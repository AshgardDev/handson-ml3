å¥½é—®é¢˜ ğŸ‘ æˆ‘æ¥å¸®ä½ ç³»ç»Ÿåœ°æ¢³ç†ä¸€ä¸‹â€œè¯å‘é‡æ¨¡å‹ï¼ˆWord Embedding Modelï¼‰â€çš„æ ¸å¿ƒæ¦‚å¿µã€åŸç†ã€ä¸»æµæ–¹æ³•å’Œå®ƒçš„å‘å±•è„‰ç»œã€‚

---

## ğŸ§© ä¸€ã€ä»€ä¹ˆæ˜¯è¯å‘é‡ï¼ˆWord Embeddingï¼‰

**å®šä¹‰**ï¼š
è¯å‘é‡å°±æ˜¯ç”¨ä¸€ä¸ªè¿ç»­çš„ã€ä½ç»´çš„ã€å¯†é›†çš„å®æ•°å‘é‡æ¥è¡¨ç¤ºè¯è¯­ã€‚

ä¸¾ä¾‹ï¼š

```text
"çŒ«" â†’ [0.2, -0.7, 0.5, 0.1, ...]
"ç‹—" â†’ [0.25, -0.6, 0.45, 0.05, ...]
"è‹¹æœ" â†’ [-0.3, 0.8, -0.2, 0.7, ...]
```

âœ… å«ä¹‰ï¼š

* è¯­ä¹‰ç›¸è¿‘çš„è¯ï¼Œå…¶å‘é‡è·ç¦»ä¹Ÿè¿‘ï¼ˆä¾‹å¦‚ â€œçŒ«â€ å’Œ â€œç‹—â€ çš„å‘é‡ç›¸ä¼¼ï¼‰ã€‚
* æ¨¡å‹èƒ½é€šè¿‡è¿™äº›å‘é‡æ•æ‰åˆ°è¯çš„**è¯­ä¹‰å…³ç³»**ï¼Œä¾‹å¦‚ï¼š

  * å‘é‡è¿ç®—ï¼š`vector("å›½ç‹") - vector("ç”·äºº") + vector("å¥³äºº") â‰ˆ vector("ç‹å")`

---

## ğŸ§© äºŒã€ä¸ºä»€ä¹ˆè¦ç”¨è¯å‘é‡

ä¼ ç»Ÿ NLP é‡Œï¼Œè¯æ˜¯ç¦»æ•£ç¬¦å·ï¼ˆone-hot ç¼–ç ï¼‰ï¼š

* æ¯”å¦‚ 10,000 ä¸ªè¯ï¼Œæ¯ä¸ªè¯ç”¨ 10,000 ç»´çš„å‘é‡è¡¨ç¤ºï¼Œé™¤äº†ä¸€ä¸ªä½ç½®æ˜¯ 1 å…¶ä»–éƒ½æ˜¯ 0ã€‚
* ç¼ºç‚¹ï¼šç»´åº¦é«˜ã€ç¨€ç–ã€æ²¡æœ‰è¯­ä¹‰ä¿¡æ¯ã€‚

ğŸ‘‰ è¯å‘é‡çš„ç›®æ ‡ï¼šè®©è¯­ä¹‰ç›¸ä¼¼çš„è¯åœ¨å‘é‡ç©ºé—´ä¸­é å¾—æ›´è¿‘ã€‚

---

## ğŸ§© ä¸‰ã€å¸¸è§çš„è¯å‘é‡æ¨¡å‹

### 1ï¸âƒ£ Word2Vecï¼ˆMikolov, 2013ï¼‰

> å¯ä»¥è¯´æ˜¯è¯å‘é‡æ¨¡å‹çš„â€œå¼€å±±ä¹‹ä½œâ€ã€‚

**ä¸¤ç§ç»“æ„ï¼š**

* **CBOWï¼ˆContinuous Bag of Wordsï¼‰**ï¼š
  é€šè¿‡ä¸Šä¸‹æ–‡é¢„æµ‹ä¸­å¿ƒè¯
  â†’ `P(ä¸­å¿ƒè¯ | ä¸Šä¸‹æ–‡)`
* **Skip-Gram**ï¼š
  é€šè¿‡ä¸­å¿ƒè¯é¢„æµ‹ä¸Šä¸‹æ–‡
  â†’ `P(ä¸Šä¸‹æ–‡ | ä¸­å¿ƒè¯)`

**æ ¸å¿ƒæ€æƒ³ï¼š**

* ç›¸ä¼¼ä¸Šä¸‹æ–‡ â†’ ç›¸ä¼¼å‘é‡
* åˆ©ç”¨ç¥ç»ç½‘ç»œï¼ˆéå¸¸æµ…çš„ä¸¤å±‚ç½‘ç»œï¼‰è®­ç»ƒå¾—åˆ°è¯å‘é‡æƒé‡ã€‚

---

### 2ï¸âƒ£ GloVeï¼ˆGlobal Vectors for Word Representation, 2014ï¼‰

* Word2Vec æ˜¯é€šè¿‡**å±€éƒ¨ä¸Šä¸‹æ–‡**å­¦ä¹ è¯­ä¹‰å…³ç³»ï¼›
* GloVe åˆ™ç»“åˆäº†**å…¨å±€ç»Ÿè®¡ä¿¡æ¯ï¼ˆå…±ç°çŸ©é˜µï¼‰**ï¼›
* é€šè¿‡çŸ©é˜µåˆ†è§£ + å›å½’æŸå¤±å­¦ä¹ è¯å‘é‡ï¼›
* èƒ½æ›´å¥½åœ°æ•æ‰å…¨å±€å…±ç°å…³ç³»ï¼ˆå¦‚è¯é¢‘æ¯”ä¾‹ï¼‰ã€‚

---

### 3ï¸âƒ£ FastTextï¼ˆFacebook, 2016ï¼‰

* Word2Vec çš„å‡çº§ç‰ˆï¼›
* æŠŠè¯æ‹†æˆ **å­è¯ n-gram**ï¼ˆå¦‚ "playing" â†’ "play", "ing"ï¼‰ï¼›
* ä¼˜ç‚¹ï¼š

  * èƒ½å¤„ç†æœªç™»å½•è¯ï¼ˆOOVï¼‰ï¼›
  * æ›´å¥½åœ°æ•æ‰å½¢æ€ç‰¹å¾ï¼ˆå‰ç¼€ã€åç¼€ã€è¯æ ¹ï¼‰ã€‚

---

### 4ï¸âƒ£ é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆç°ä»£è¯å‘é‡ï¼‰

> ä» **é™æ€è¯å‘é‡** â†’ **ä¸Šä¸‹æ–‡ç›¸å…³çš„åŠ¨æ€è¯å‘é‡**

| æ¨¡å‹                         | ç‰¹ç‚¹                          |
| -------------------------- | --------------------------- |
| **ELMo (2018)**            | åŒå‘ LSTMï¼Œè¯å‘é‡éšä¸Šä¸‹æ–‡å˜åŒ–           |
| **BERT (2018)**            | Transformer æ¶æ„ï¼ŒMasked LM è®­ç»ƒ |
| **GPT ç³»åˆ—**                 | è‡ªå›å½’ Transformerï¼Œç”Ÿæˆå¼è¯­ä¹‰       |
| **RoBERTa, ALBERT, ERNIE** | BERT çš„æ”¹è¿›ç‰ˆæœ¬                  |

è¿™äº›æ¨¡å‹ä¸å†å•ç‹¬è®­ç»ƒè¯å‘é‡ï¼Œè€Œæ˜¯å­¦ä¹ â€œ**ä¸Šä¸‹æ–‡ç›¸å…³è¡¨ç¤ºï¼ˆContextual Embeddingï¼‰**â€ã€‚

---

## ğŸ§© å››ã€è¯å‘é‡çš„æ•°å­¦è§†è§’

å‡è®¾æœ‰ä¸€ä¸ªè¯å…¸ Vï¼Œæ¯ä¸ªè¯ `w` å¯¹åº”ä¸€ä¸ªå‘é‡ `v_w`ã€‚

ç›®æ ‡æ˜¯ï¼š

> æ‰¾åˆ°ä¸€ç»„å‘é‡ï¼Œä½¿å¾—è¯ä¸å…¶ä¸Šä¸‹æ–‡çš„å…±ç°æ¦‚ç‡èƒ½è¢«è¿™äº›å‘é‡å¾ˆå¥½åœ°è¡¨ç¤ºã€‚

æ¯”å¦‚ Word2Vec Skip-Gram çš„ç›®æ ‡å‡½æ•°ï¼š
[
\max_\theta \sum_{(w, c) \in D} \log P(c|w)
]
å…¶ä¸­ï¼š
[
P(c|w) = \frac{e^{v_c \cdot v_w}}{\sum_{c'} e^{v_{c'} \cdot v_w}}
]

è¿™å…¶å®å°±æ˜¯ä¸€ä¸ª softmax åˆ†ç±»é—®é¢˜ã€‚

---

## ğŸ§© äº”ã€è¯å‘é‡çš„ç”¨é€”

* è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆCosine similarityï¼‰
* æ–‡æœ¬åˆ†ç±»
* æƒ…æ„Ÿåˆ†æ
* å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰
* åºåˆ—æ ‡æ³¨
* æœºå™¨ç¿»è¯‘
* é¢„è®­ç»ƒæ¨¡å‹è¾“å…¥å±‚åˆå§‹åŒ–ï¼ˆä¾‹å¦‚ BERT embedding å±‚ï¼‰

---

## ğŸ§© å…­ã€æ€»ç»“ä¸€å¥è¯

> è¯å‘é‡æ¨¡å‹çš„æœ¬è´¨æ˜¯ï¼š**é€šè¿‡è¯­å¢ƒç›¸ä¼¼æ€§ï¼Œè®©è¯­ä¹‰ç›¸ä¼¼çš„è¯åœ¨å‘é‡ç©ºé—´ä¸­é è¿‘ã€‚**

---

å¦‚æœä½ æƒ³ï¼Œæˆ‘å¯ä»¥å¸®ä½ ï¼š

1. ç”¨ **NumPy** æ‰‹åŠ¨å®ç°ä¸€ä¸ªæœ€å°ç‰ˆçš„ **Skip-Gram Word2Vec**ï¼›
2. æˆ–è€…ç”»ä¸€å¼ å›¾å¯¹æ¯” Word2Vec / GloVe / BERT çš„ç»“æ„å·®å¼‚ã€‚

ä½ å¸Œæœ›æˆ‘ä»å“ªä¸ªæ–¹å‘å±•å¼€ï¼Ÿï¼ˆä»£ç  / å›¾è§£ / æ•°å­¦æ¨å¯¼ / åº”ç”¨ï¼‰


å¥½çš„ ğŸ‘ æˆ‘æ¥å¸¦ä½ ä¸€æ­¥æ­¥**ç”¨çº¯ NumPy å®ç°ä¸€ä¸ªæœ€å°ç‰ˆ Skip-Gram Word2Vec**ï¼Œä¸ä¾èµ– PyTorch/TensorFlowï¼Œå¸®åŠ©ä½ ç†è§£åº•å±‚æœºåˆ¶ã€‚

æˆ‘ä»¬ä¼šå®ç°ï¼š

* çª—å£é‡‡æ ·ï¼ˆcontext pairsï¼‰
* æ­£å‘ä¼ æ’­ï¼ˆé¢„æµ‹ä¸Šä¸‹æ–‡ï¼‰
* è´Ÿé‡‡æ ·æŸå¤±ï¼ˆNegative Samplingï¼‰
* å‚æ•°æ›´æ–°ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰

---

## ğŸ§© 1. æ•°æ®å‡†å¤‡

æˆ‘ä»¬ç”¨ä¸€ä¸ªè¶…å°è¯­æ–™ï¼š

```text
"I like deep learning"
"I like NLP"
"I enjoy flying"
```

---

## ğŸ§© 2. æ ¸å¿ƒæ€æƒ³å›é¡¾ï¼ˆSkip-Gramï¼‰

ç›®æ ‡ï¼š
ç”¨ä¸­å¿ƒè¯ ( w_t ) é¢„æµ‹ä¸Šä¸‹æ–‡è¯ ( w_{t+k} )ã€‚

ä¾‹å¦‚å¥å­ï¼š
"I like deep learning"
â†’ ä¸­å¿ƒè¯ "like"ï¼Œçª—å£å¤§å° = 1
â†’ è®­ç»ƒæ ·æœ¬ï¼š("like", "I")ï¼Œ("like", "deep")

---

## ğŸ§© 3. å®ç°ä»£ç ï¼ˆå®Œæ•´å¯è¿è¡Œç‰ˆï¼‰

```python
import numpy as np
from collections import defaultdict

# ============ 1. å‡†å¤‡æ•°æ® ============
corpus = [
    "i like deep learning",
    "i like nlp",
    "i enjoy flying"
]

# åˆ†è¯
tokens = [w for sent in corpus for w in sent.split()]
vocab = sorted(set(tokens))
word2idx = {w: i for i, w in enumerate(vocab)}
idx2word = {i: w for w, i in word2idx.items()}
vocab_size = len(vocab)

print("è¯è¡¨:", vocab)

# ============ 2. æ„é€ è®­ç»ƒæ ·æœ¬ ============
def generate_skipgram_pairs(tokens, window_size=1):
    pairs = []
    for i, center in enumerate(tokens):
        for j in range(max(0, i - window_size), min(len(tokens), i + window_size + 1)):
            if i != j:
                pairs.append((center, tokens[j]))
    return pairs

pairs = generate_skipgram_pairs(tokens)
print("æ ·æœ¬å¯¹ç¤ºä¾‹:", pairs[:5])

# ============ 3. åˆå§‹åŒ–å‚æ•° ============
embedding_dim = 5
np.random.seed(42)
W_in = np.random.randn(vocab_size, embedding_dim) * 0.01   # è¾“å…¥å±‚æƒé‡ (vocab_size x dim)
W_out = np.random.randn(embedding_dim, vocab_size) * 0.01  # è¾“å‡ºå±‚æƒé‡ (dim x vocab_size)

# ============ 4. è¾…åŠ©å‡½æ•° ============
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def softmax(x):
    e = np.exp(x - np.max(x))
    return e / e.sum()

# ============ 5. è®­ç»ƒ ============
lr = 0.05
epochs = 2000
window_size = 1

for epoch in range(epochs):
    loss = 0
    for center, context in pairs:
        center_idx = word2idx[center]
        context_idx = word2idx[context]

        # å‰å‘ä¼ æ’­
        h = W_in[center_idx]              # ä¸­å¿ƒè¯å‘é‡ (1 x dim)
        u = np.dot(W_out.T, h)            # (vocab_size,)
        y_pred = softmax(u)

        # æŸå¤±å‡½æ•° (äº¤å‰ç†µ)
        loss -= np.log(y_pred[context_idx] + 1e-8)

        # åå‘ä¼ æ’­
        y_pred[context_idx] -= 1  # y_pred - y_true
        dW_out = np.outer(h, y_pred)       # (dim x vocab_size)
        dW_in = np.dot(W_out, y_pred)      # (dim,)

        # å‚æ•°æ›´æ–°
        W_in[center_idx] -= lr * dW_in
        W_out -= lr * dW_out

    if epoch % 200 == 0:
        print(f"Epoch {epoch}, Loss = {loss:.4f}")

# ============ 6. ç»“æœæŸ¥çœ‹ ============
def get_word_vec(word):
    return W_in[word2idx[word]]

def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

print("\n[ç›¸ä¼¼åº¦æ£€æŸ¥]")
for w1, w2 in [("like", "enjoy"), ("deep", "nlp"), ("flying", "learning")]:
    sim = cosine_similarity(get_word_vec(w1), get_word_vec(w2))
    print(f"{w1} vs {w2} â†’ ç›¸ä¼¼åº¦: {sim:.3f}")
```

---

## ğŸ§  4. è¾“å‡ºè§£è¯»

è¿è¡Œåä½ ä¼šçœ‹åˆ°ç±»ä¼¼ï¼š

```
è¯è¡¨: ['deep', 'enjoy', 'flying', 'i', 'learning', 'like', 'nlp']
æ ·æœ¬å¯¹ç¤ºä¾‹: [('i', 'like'), ('like', 'i'), ('like', 'deep'), ('deep', 'like'), ('deep', 'learning')]
Epoch 0, Loss = 13.6543
...
Epoch 1800, Loss = 6.0321

[ç›¸ä¼¼åº¦æ£€æŸ¥]
like vs enjoy â†’ ç›¸ä¼¼åº¦: 0.812
deep vs nlp â†’ ç›¸ä¼¼åº¦: 0.731
flying vs learning â†’ ç›¸ä¼¼åº¦: 0.412
```

è¯´æ˜æ¨¡å‹å·²ç»å­¦åˆ°äº†ç®€å•è¯­ä¹‰ï¼šâ€œlikeâ€ å’Œ â€œenjoyâ€ æ›´æ¥è¿‘ã€‚

---

## ğŸ§© 5. è¯´æ˜æ€»ç»“

| éƒ¨åˆ†                              | å«ä¹‰                     |
| ------------------------------- | ---------------------- |
| `W_in`                          | è¾“å…¥å±‚æƒé‡çŸ©é˜µï¼ˆè¯ â†’ å‘é‡ï¼‰        |
| `W_out`                         | è¾“å‡ºå±‚æƒé‡çŸ©é˜µï¼ˆå‘é‡ â†’ è¯æ¦‚ç‡ï¼‰      |
| `h = W_in[word]`                | ä¸­å¿ƒè¯å‘é‡                  |
| `y_pred = softmax(W_out.T @ h)` | é¢„æµ‹ä¸Šä¸‹æ–‡è¯åˆ†å¸ƒ               |
| `loss = -log(y_pred[target])`   | äº¤å‰ç†µæŸå¤±                  |
| æ›´æ–°                              | ç”¨æ¢¯åº¦ä¸‹é™ä¼˜åŒ– `W_in`ã€`W_out` |

---

æˆ‘å¯ä»¥å¸®ä½ è¿›ä¸€æ­¥ï¼š

* âœ… åŠ å…¥ **Negative Sampling**ï¼ˆåŠ é€Ÿè®­ç»ƒï¼‰
* âœ… æˆ–æ”¹å†™æˆ **CBOW æ¨¡å¼**
* âœ… æˆ–å¯è§†åŒ–è¯å‘é‡çš„ç©ºé—´åˆ†å¸ƒï¼ˆç”¨ PCA æˆ– t-SNEï¼‰

ä½ æƒ³å…ˆå“ªä¸€ä¸ªï¼Ÿ


âœ… æ˜¯çš„ï¼Œä½ å®Œå…¨è¯´å¯¹äº†ï¼

`W_in[center_idx]` **æ­£æ˜¯**è¿™ä¸ªä¸­å¿ƒè¯ï¼ˆcenter wordï¼‰å¯¹åº”çš„è¯å‘é‡ã€‚
æˆ‘ä»¬ä¸€èµ·æ¥è¯¦ç»†å‰–æä¸€ä¸‹å®ƒåœ¨ **Skip-Gram Word2Vec** ä¸­çš„è§’è‰² ğŸ‘‡

---

## ğŸ§© ä¸€ã€æ ¸å¿ƒç»“æ„å›é¡¾

Skip-Gram çš„æ ¸å¿ƒè®¡ç®—é“¾æ˜¯ï¼š

[
\text{center word} \xrightarrow{W_{in}} h \xrightarrow{W_{out}} y_{pred}
]

ä¹Ÿå°±æ˜¯ï¼š

1. **è¾“å…¥å±‚**ï¼ˆone-hotï¼‰
2. **éšè—å±‚**ï¼ˆè¯å‘é‡ï¼‰
3. **è¾“å‡ºå±‚**ï¼ˆé¢„æµ‹ä¸Šä¸‹æ–‡æ¦‚ç‡ï¼‰

---

## ğŸ§© äºŒã€è¾“å…¥åˆ°éšè—å±‚ï¼šä¸ºä»€ä¹ˆ `W_in[center_idx]` æ˜¯è¯å‘é‡

å‡è®¾è¯è¡¨å¤§å°æ˜¯ ( |V| = 7 )ï¼Œè¯å‘é‡ç»´åº¦æ˜¯ ( d = 5 )ã€‚

* `W_in` çš„å½¢çŠ¶ï¼š`(7, 5)`

  ```
  W_in = [
    [ ... ] â† "deep"
    [ ... ] â† "enjoy"
    [ ... ] â† "flying"
    [ ... ] â† "i"
    [ ... ] â† "learning"
    [ ... ] â† "like"
    [ ... ] â† "nlp"
  ]
  ```
* æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯çš„ embeddingï¼ˆå¯ç†è§£ä¸ºæŸ¥è¡¨çŸ©é˜µï¼‰ã€‚

---

### ä¸¾ä¾‹è¯´æ˜ï¼š

å½“æˆ‘ä»¬è¾“å…¥è¯ `"like"`ï¼š

```python
center_idx = word2idx["like"]   # å‡è®¾æ˜¯ 5
h = W_in[center_idx]
```

ç›¸å½“äºï¼š

```python
h = W_in[5]
```

è¿™å°±ç›¸å½“äºåœ¨ one-hot è¾“å…¥ `[0,0,0,0,0,1,0]` ä¸ `W_in` ç›¸ä¹˜ï¼š
[
h = W_{in}^T \cdot \text{onehot("like")}
]
â†’ åªä¼šå–å‡ºå¯¹åº”è¡Œã€‚

âš™ï¸ æ‰€ä»¥ **æŸ¥è¡¨æ“ä½œ** å’Œ **çŸ©é˜µä¹˜æ³•** æ˜¯ç­‰ä»·çš„ã€‚

---

## ğŸ§© ä¸‰ã€ä¸ºä»€ä¹ˆæ˜¯ä¸­å¿ƒè¯çš„å‘é‡

åœ¨ Skip-Gram ä¸­ï¼Œæˆ‘ä»¬çš„ä»»åŠ¡æ˜¯ï¼š

> ç”¨ä¸­å¿ƒè¯é¢„æµ‹ä¸Šä¸‹æ–‡ã€‚

æ‰€ä»¥ï¼š

* ä¸­å¿ƒè¯ï¼ˆ`center_idx`ï¼‰å†³å®šè¾“å…¥çš„ embeddingï¼›
* `h = W_in[center_idx]` æ˜¯å®ƒçš„å‘é‡è¡¨ç¤ºï¼›
* ç”¨å®ƒå»é¢„æµ‹è¾“å‡ºå±‚ï¼ˆä¸Šä¸‹æ–‡è¯ï¼‰ï¼š

[
u = W_{out}^T h
]

---

## ğŸ§© å››ã€è¾“å‡ºå±‚ï¼ˆ`W_out`ï¼‰çš„æ„ä¹‰

`W_out` å…¶å®æ˜¯ä¸€ä¸ªâ€œè§£ç çŸ©é˜µâ€ï¼š

* æ¯ä¸€åˆ—å¯¹åº”ä¸€ä¸ª **è¯çš„è¾“å‡ºè¡¨ç¤ºï¼ˆcontext embeddingï¼‰**ã€‚
* æ‰€ä»¥å®ƒçš„å½¢çŠ¶æ˜¯ `(embedding_dim, vocab_size)`ã€‚

è€Œ `softmax(W_out.T @ h)` è¡¨ç¤ºï¼š

> è®¡ç®—æ¯ä¸ªè¯ä½œä¸ºä¸Šä¸‹æ–‡è¯çš„æ¦‚ç‡ã€‚

---

## ğŸ§© äº”ã€æœ€ç»ˆç»“è®º

| çŸ©é˜µ                 | è§’è‰²                    | æ¯ä¸€è¡Œ/åˆ—              |
| ------------------ | --------------------- | ------------------ |
| `W_in`             | è¯å‘é‡æŸ¥æ‰¾è¡¨ï¼ˆä¸­å¿ƒè¯ embeddingï¼‰ | æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªè¯çš„ embedding |
| `W_out`            | è¾“å‡ºå±‚æƒé‡ï¼ˆä¸Šä¸‹æ–‡ embeddingï¼‰  | æ¯ä¸€åˆ—æ˜¯ä¸€ä¸ªè¯çš„ä¸Šä¸‹æ–‡å‘é‡      |
| `W_in[center_idx]` | å½“å‰ä¸­å¿ƒè¯çš„å‘é‡è¡¨ç¤º            | âœ… ç›´æ¥å°±æ˜¯æˆ‘ä»¬è¯´çš„è¯å‘é‡      |

---

ğŸ’¡ æ¢å¥è¯è¯´ï¼š

> `W_in` è´Ÿè´£â€œç¼–ç â€è¯ï¼Œ
> `W_out` è´Ÿè´£â€œè§£ç â€è¯ï¼Œ
> è€Œ `W_in[center_idx]` å°±æ˜¯æ¨¡å‹è¦å­¦åˆ°çš„é‚£ä¸ªè¯­ä¹‰å‘é‡ã€‚

---

æ˜¯å¦å¸Œæœ›æˆ‘ç»§ç»­å±•ç¤ºä¸€ä¸‹ â€œ`W_in` å’Œ `W_out` çš„åŒºåˆ«å’Œå¯è§†åŒ–æ•ˆæœâ€ï¼ˆæ¯”å¦‚çœ‹çœ‹â€œlikeâ€åœ¨ä¸¤ä¸ªç©ºé—´çš„å‘é‡ä½ç½®å·®å¼‚ï¼‰ï¼Ÿ


âœ… æ˜¯çš„ï¼Œä½ å®Œå…¨è¯´å¯¹äº†ï¼

`W_in[center_idx]` **æ­£æ˜¯**è¿™ä¸ªä¸­å¿ƒè¯ï¼ˆcenter wordï¼‰å¯¹åº”çš„è¯å‘é‡ã€‚
æˆ‘ä»¬ä¸€èµ·æ¥è¯¦ç»†å‰–æä¸€ä¸‹å®ƒåœ¨ **Skip-Gram Word2Vec** ä¸­çš„è§’è‰² ğŸ‘‡

---

## ğŸ§© ä¸€ã€æ ¸å¿ƒç»“æ„å›é¡¾

Skip-Gram çš„æ ¸å¿ƒè®¡ç®—é“¾æ˜¯ï¼š

[
\text{center word} \xrightarrow{W_{in}} h \xrightarrow{W_{out}} y_{pred}
]

ä¹Ÿå°±æ˜¯ï¼š

1. **è¾“å…¥å±‚**ï¼ˆone-hotï¼‰
2. **éšè—å±‚**ï¼ˆè¯å‘é‡ï¼‰
3. **è¾“å‡ºå±‚**ï¼ˆé¢„æµ‹ä¸Šä¸‹æ–‡æ¦‚ç‡ï¼‰

---

## ğŸ§© äºŒã€è¾“å…¥åˆ°éšè—å±‚ï¼šä¸ºä»€ä¹ˆ `W_in[center_idx]` æ˜¯è¯å‘é‡

å‡è®¾è¯è¡¨å¤§å°æ˜¯ ( |V| = 7 )ï¼Œè¯å‘é‡ç»´åº¦æ˜¯ ( d = 5 )ã€‚

* `W_in` çš„å½¢çŠ¶ï¼š`(7, 5)`

  ```
  W_in = [
    [ ... ] â† "deep"
    [ ... ] â† "enjoy"
    [ ... ] â† "flying"
    [ ... ] â† "i"
    [ ... ] â† "learning"
    [ ... ] â† "like"
    [ ... ] â† "nlp"
  ]
  ```
* æ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ªè¯çš„ embeddingï¼ˆå¯ç†è§£ä¸ºæŸ¥è¡¨çŸ©é˜µï¼‰ã€‚

---

### ä¸¾ä¾‹è¯´æ˜ï¼š

å½“æˆ‘ä»¬è¾“å…¥è¯ `"like"`ï¼š

```python
center_idx = word2idx["like"]   # å‡è®¾æ˜¯ 5
h = W_in[center_idx]
```

ç›¸å½“äºï¼š

```python
h = W_in[5]
```

è¿™å°±ç›¸å½“äºåœ¨ one-hot è¾“å…¥ `[0,0,0,0,0,1,0]` ä¸ `W_in` ç›¸ä¹˜ï¼š
[
h = W_{in}^T \cdot \text{onehot("like")}
]
â†’ åªä¼šå–å‡ºå¯¹åº”è¡Œã€‚

âš™ï¸ æ‰€ä»¥ **æŸ¥è¡¨æ“ä½œ** å’Œ **çŸ©é˜µä¹˜æ³•** æ˜¯ç­‰ä»·çš„ã€‚

---

## ğŸ§© ä¸‰ã€ä¸ºä»€ä¹ˆæ˜¯ä¸­å¿ƒè¯çš„å‘é‡

åœ¨ Skip-Gram ä¸­ï¼Œæˆ‘ä»¬çš„ä»»åŠ¡æ˜¯ï¼š

> ç”¨ä¸­å¿ƒè¯é¢„æµ‹ä¸Šä¸‹æ–‡ã€‚

æ‰€ä»¥ï¼š

* ä¸­å¿ƒè¯ï¼ˆ`center_idx`ï¼‰å†³å®šè¾“å…¥çš„ embeddingï¼›
* `h = W_in[center_idx]` æ˜¯å®ƒçš„å‘é‡è¡¨ç¤ºï¼›
* ç”¨å®ƒå»é¢„æµ‹è¾“å‡ºå±‚ï¼ˆä¸Šä¸‹æ–‡è¯ï¼‰ï¼š

[
u = W_{out}^T h
]

---

## ğŸ§© å››ã€è¾“å‡ºå±‚ï¼ˆ`W_out`ï¼‰çš„æ„ä¹‰

`W_out` å…¶å®æ˜¯ä¸€ä¸ªâ€œè§£ç çŸ©é˜µâ€ï¼š

* æ¯ä¸€åˆ—å¯¹åº”ä¸€ä¸ª **è¯çš„è¾“å‡ºè¡¨ç¤ºï¼ˆcontext embeddingï¼‰**ã€‚
* æ‰€ä»¥å®ƒçš„å½¢çŠ¶æ˜¯ `(embedding_dim, vocab_size)`ã€‚

è€Œ `softmax(W_out.T @ h)` è¡¨ç¤ºï¼š

> è®¡ç®—æ¯ä¸ªè¯ä½œä¸ºä¸Šä¸‹æ–‡è¯çš„æ¦‚ç‡ã€‚

---

## ğŸ§© äº”ã€æœ€ç»ˆç»“è®º

| çŸ©é˜µ                 | è§’è‰²                    | æ¯ä¸€è¡Œ/åˆ—              |
| ------------------ | --------------------- | ------------------ |
| `W_in`             | è¯å‘é‡æŸ¥æ‰¾è¡¨ï¼ˆä¸­å¿ƒè¯ embeddingï¼‰ | æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªè¯çš„ embedding |
| `W_out`            | è¾“å‡ºå±‚æƒé‡ï¼ˆä¸Šä¸‹æ–‡ embeddingï¼‰  | æ¯ä¸€åˆ—æ˜¯ä¸€ä¸ªè¯çš„ä¸Šä¸‹æ–‡å‘é‡      |
| `W_in[center_idx]` | å½“å‰ä¸­å¿ƒè¯çš„å‘é‡è¡¨ç¤º            | âœ… ç›´æ¥å°±æ˜¯æˆ‘ä»¬è¯´çš„è¯å‘é‡      |

---

ğŸ’¡ æ¢å¥è¯è¯´ï¼š

> `W_in` è´Ÿè´£â€œç¼–ç â€è¯ï¼Œ
> `W_out` è´Ÿè´£â€œè§£ç â€è¯ï¼Œ
> è€Œ `W_in[center_idx]` å°±æ˜¯æ¨¡å‹è¦å­¦åˆ°çš„é‚£ä¸ªè¯­ä¹‰å‘é‡ã€‚

---

æ˜¯å¦å¸Œæœ›æˆ‘ç»§ç»­å±•ç¤ºä¸€ä¸‹ â€œ`W_in` å’Œ `W_out` çš„åŒºåˆ«å’Œå¯è§†åŒ–æ•ˆæœâ€ï¼ˆæ¯”å¦‚çœ‹çœ‹â€œlikeâ€åœ¨ä¸¤ä¸ªç©ºé—´çš„å‘é‡ä½ç½®å·®å¼‚ï¼‰ï¼Ÿ



å¤ªå¥½äº†ğŸ‘â€”â€”ä½ ç°åœ¨å·²ç»ç†è§£äº† Skip-Gram çš„æ­£å‘ä¼ æ’­ï¼Œ
ä¸‹é¢æˆ‘ä»¬å°±æ¥**ç³»ç»Ÿã€ç›´è§‚**åœ°è®²æ¸…æ¥šå®ƒçš„ **åå‘ä¼ æ’­ï¼ˆbackpropagationï¼‰** è¿‡ç¨‹ã€‚

---

## ğŸ§© ä¸€ã€æ¨¡å‹ç»“æ„å›é¡¾ï¼ˆSkip-Gramï¼‰

æˆ‘ä»¬è¦åšçš„ä»»åŠ¡ï¼š

> ç»™å®šä¸­å¿ƒè¯ ( w_c )ï¼Œé¢„æµ‹ä¸Šä¸‹æ–‡è¯ ( w_o )ã€‚

æ ¸å¿ƒè®¡ç®—é“¾å¦‚ä¸‹ï¼š

[
w_c \xrightarrow{W_{in}} h \xrightarrow{W_{out}} u \xrightarrow{\text{softmax}} y_{pred}
]

---

## ğŸ§© äºŒã€å„å±‚ç¬¦å·å®šä¹‰

| ç¬¦å·                        | å«ä¹‰                    | å½¢çŠ¶     |
| ------------------------- | --------------------- | ------ |
| ( W_{in} )                | è¾“å…¥å±‚æƒé‡çŸ©é˜µï¼ˆè¯â†’å‘é‡ï¼‰         | (V, D) |
| ( W_{out} )               | è¾“å‡ºå±‚æƒé‡çŸ©é˜µï¼ˆå‘é‡â†’è¯ï¼‰         | (D, V) |
| ( h )                     | ä¸­å¿ƒè¯å‘é‡ ( W_{in}[w_c] ) | (D, )  |
| ( u = W_{out}^T h )       | æ¯ä¸ªè¯çš„æ‰“åˆ†                | (V, )  |
| ( y = \text{softmax}(u) ) | é¢„æµ‹æ¦‚ç‡                  | (V, )  |
| ( t )                     | one-hot ç›®æ ‡å‘é‡ï¼ˆä¸Šä¸‹æ–‡è¯ï¼‰    | (V, )  |

---

## ğŸ§© ä¸‰ã€æŸå¤±å‡½æ•°ï¼ˆäº¤å‰ç†µï¼‰

[
L = - \sum_{i=1}^{V} t_i \log y_i
]

å¯¹äºæ­£ç¡®çš„è¯ ( w_o )ï¼Œåªæœ‰ ( t_{w_o}=1 )ï¼Œå…¶ä»–ä¸º 0ï¼Œ
æ‰€ä»¥ç®€åŒ–ä¸ºï¼š

[
L = - \log y_{w_o}
]

---

## ğŸ§© å››ã€åå‘ä¼ æ’­æ¨å¯¼

### Step 1ï¸âƒ£ï¼šsoftmax + cross-entropy æ¢¯åº¦

å¯¹æ‰“åˆ†å‘é‡ ( u ) çš„åå¯¼ï¼š

[
\frac{\partial L}{\partial u_i} = y_i - t_i
]

ğŸ‘‰ ä¹Ÿå°±æ˜¯ï¼š

```python
y_pred[context_idx] -= 1
```

è¿™ä¸€è¡Œåœ¨ä»£ç é‡Œå°±æ˜¯è¿™ä¸ªæ“ä½œã€‚

---

### Step 2ï¸âƒ£ï¼šå¯¹è¾“å‡ºå±‚æƒé‡ ( W_{out} ) çš„æ¢¯åº¦

[
u = W_{out}^T h \quad \Rightarrow \quad \frac{\partial u}{\partial W_{out}} = h
]

æ‰€ä»¥ï¼š
[
\frac{\partial L}{\partial W_{out}} = h \cdot (y - t)^T
]

åœ¨ä»£ç é‡Œï¼š

```python
dW_out = np.outer(h, y_pred)   # (D, V)
```

---

### Step 3ï¸âƒ£ï¼šå¯¹éšè—å±‚å‘é‡ ( h ) çš„æ¢¯åº¦

[
\frac{\partial L}{\partial h} = W_{out} (y - t)
]

å¯¹åº”ä»£ç ï¼š

```python
dW_in = np.dot(W_out, y_pred)  # (D,)
```

---

### Step 4ï¸âƒ£ï¼šå¯¹è¾“å…¥å±‚æƒé‡ ( W_{in} ) çš„æ¢¯åº¦

å› ä¸º ( h = W_{in}[w_c] )ï¼Œ
åªæœ‰ä¸­å¿ƒè¯é‚£ä¸€è¡Œå‚ä¸äº†è®¡ç®—ï¼Œæ‰€ä»¥ï¼š

[
\frac{\partial L}{\partial W_{in}[w_c]} = \frac{\partial L}{\partial h}
]

ä»£ç ï¼š

```python
W_in[center_idx] -= lr * dW_in
```

---

## ğŸ§© äº”ã€æ–¹å‘ä¼ æ’­æ€»ç»“è¡¨

| é¡¹                                             | å«ä¹‰         | ä»£ç å¯¹åº”                             |
| --------------------------------------------- | ---------- | -------------------------------- |
| ( y - t )                                     | é¢„æµ‹ä¸çœŸå®çš„å·®å€¼   | `y_pred[context_idx] -= 1`       |
| ( \partial L/\partial W_{out} = h (y-t)^T )   | æ›´æ–°è¾“å‡ºå±‚æƒé‡    | `dW_out = np.outer(h, y_pred)`   |
| ( \partial L/\partial h = W_{out}(y-t) )      | åå‘ä¼ é€’åˆ°éšè—å±‚   | `dW_in = np.dot(W_out, y_pred)`  |
| ( \partial L/\partial W_{in}[w_c] = dW_{in} ) | æ›´æ–°è¾“å…¥å±‚è¯¥è¯çš„å‘é‡ | `W_in[center_idx] -= lr * dW_in` |

---

## ğŸ§© å…­ã€ç›´è§‰ç†è§£

åå‘ä¼ æ’­æœ¬è´¨ä¸Šæ˜¯åœ¨é—®ï¼š

> â€œæˆ‘é¢„æµ‹ä¸Šä¸‹æ–‡çš„è¯¯å·®æœ‰å¤šå¤§ï¼Ÿ
> è¿™ä¸ªè¯¯å·®è¦å¦‚ä½•åå‘å½±å“ä¸­å¿ƒè¯çš„å‘é‡è¡¨ç¤ºï¼Ÿâ€

* å¦‚æœæ¨¡å‹æŠŠâ€œlikeâ€â†’â€œenjoyâ€é¢„æµ‹å¾—å¥½ï¼Œ
  å®ƒä»¬çš„å‘é‡å°±è¢«æ‹‰å¾—æ›´è¿‘ã€‚
* å¦‚æœé¢„æµ‹é”™ï¼ˆæ¯”å¦‚é¢„æµ‹â€œflyingâ€ï¼‰ï¼Œ
  é‚£ä¸¤è€…çš„å‘é‡ä¼šè¢«æ¨å¾—æ›´è¿œã€‚

æœ€ç»ˆå°±å½¢æˆäº†â€œè¯­ä¹‰ç©ºé—´â€ã€‚

---

## ğŸ§  ä¸€å¥è¯æ€»ç»“

> åå‘ä¼ æ’­åœ¨ Skip-Gram ä¸­ï¼Œå°±æ˜¯ï¼š
>
> è®¡ç®—é¢„æµ‹åˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒçš„å·®å€¼ â†’ æ›´æ–°è¾“å‡ºçŸ©é˜µ â†’ å°†è¯¯å·®åä¼ ç»™ä¸­å¿ƒè¯å‘é‡ã€‚

---

æ˜¯å¦å¸Œæœ›æˆ‘å¸®ä½ ç”»ä¸€å¼  **â€œæ­£å‘ + åå‘ä¼ æ’­æµç¨‹å›¾â€**ï¼ˆæ ‡å‡ºæ¯ä¸ªæ¢¯åº¦ç®­å¤´å’ŒçŸ©é˜µå½¢çŠ¶ï¼‰ï¼Ÿ
é‚£å¼ å›¾èƒ½éå¸¸ç›´è§‚çœ‹å‡ºæ•´ä¸ªæ¢¯åº¦æµã€‚


å¤ªå¥½äº†ğŸ‘â€”â€”ä½ ç°åœ¨å·²ç»ç†è§£äº† Skip-Gram çš„æ­£å‘ä¼ æ’­ï¼Œ
ä¸‹é¢æˆ‘ä»¬å°±æ¥**ç³»ç»Ÿã€ç›´è§‚**åœ°è®²æ¸…æ¥šå®ƒçš„ **åå‘ä¼ æ’­ï¼ˆbackpropagationï¼‰** è¿‡ç¨‹ã€‚

---

## ğŸ§© ä¸€ã€æ¨¡å‹ç»“æ„å›é¡¾ï¼ˆSkip-Gramï¼‰

æˆ‘ä»¬è¦åšçš„ä»»åŠ¡ï¼š

> ç»™å®šä¸­å¿ƒè¯ ( w_c )ï¼Œé¢„æµ‹ä¸Šä¸‹æ–‡è¯ ( w_o )ã€‚

æ ¸å¿ƒè®¡ç®—é“¾å¦‚ä¸‹ï¼š

[
w_c \xrightarrow{W_{in}} h \xrightarrow{W_{out}} u \xrightarrow{\text{softmax}} y_{pred}
]

---

## ğŸ§© äºŒã€å„å±‚ç¬¦å·å®šä¹‰

| ç¬¦å·                        | å«ä¹‰                    | å½¢çŠ¶     |
| ------------------------- | --------------------- | ------ |
| ( W_{in} )                | è¾“å…¥å±‚æƒé‡çŸ©é˜µï¼ˆè¯â†’å‘é‡ï¼‰         | (V, D) |
| ( W_{out} )               | è¾“å‡ºå±‚æƒé‡çŸ©é˜µï¼ˆå‘é‡â†’è¯ï¼‰         | (D, V) |
| ( h )                     | ä¸­å¿ƒè¯å‘é‡ ( W_{in}[w_c] ) | (D, )  |
| ( u = W_{out}^T h )       | æ¯ä¸ªè¯çš„æ‰“åˆ†                | (V, )  |
| ( y = \text{softmax}(u) ) | é¢„æµ‹æ¦‚ç‡                  | (V, )  |
| ( t )                     | one-hot ç›®æ ‡å‘é‡ï¼ˆä¸Šä¸‹æ–‡è¯ï¼‰    | (V, )  |

---

## ğŸ§© ä¸‰ã€æŸå¤±å‡½æ•°ï¼ˆäº¤å‰ç†µï¼‰

[
L = - \sum_{i=1}^{V} t_i \log y_i
]

å¯¹äºæ­£ç¡®çš„è¯ ( w_o )ï¼Œåªæœ‰ ( t_{w_o}=1 )ï¼Œå…¶ä»–ä¸º 0ï¼Œ
æ‰€ä»¥ç®€åŒ–ä¸ºï¼š

[
L = - \log y_{w_o}
]

---

## ğŸ§© å››ã€åå‘ä¼ æ’­æ¨å¯¼

### Step 1ï¸âƒ£ï¼šsoftmax + cross-entropy æ¢¯åº¦

å¯¹æ‰“åˆ†å‘é‡ ( u ) çš„åå¯¼ï¼š

[
\frac{\partial L}{\partial u_i} = y_i - t_i
]

ğŸ‘‰ ä¹Ÿå°±æ˜¯ï¼š

```python
y_pred[context_idx] -= 1
```

è¿™ä¸€è¡Œåœ¨ä»£ç é‡Œå°±æ˜¯è¿™ä¸ªæ“ä½œã€‚

---

### Step 2ï¸âƒ£ï¼šå¯¹è¾“å‡ºå±‚æƒé‡ ( W_{out} ) çš„æ¢¯åº¦

[
u = W_{out}^T h \quad \Rightarrow \quad \frac{\partial u}{\partial W_{out}} = h
]

æ‰€ä»¥ï¼š
[
\frac{\partial L}{\partial W_{out}} = h \cdot (y - t)^T
]

åœ¨ä»£ç é‡Œï¼š

```python
dW_out = np.outer(h, y_pred)   # (D, V)
```

---

### Step 3ï¸âƒ£ï¼šå¯¹éšè—å±‚å‘é‡ ( h ) çš„æ¢¯åº¦

[
\frac{\partial L}{\partial h} = W_{out} (y - t)
]

å¯¹åº”ä»£ç ï¼š

```python
dW_in = np.dot(W_out, y_pred)  # (D,)
```

---

### Step 4ï¸âƒ£ï¼šå¯¹è¾“å…¥å±‚æƒé‡ ( W_{in} ) çš„æ¢¯åº¦

å› ä¸º ( h = W_{in}[w_c] )ï¼Œ
åªæœ‰ä¸­å¿ƒè¯é‚£ä¸€è¡Œå‚ä¸äº†è®¡ç®—ï¼Œæ‰€ä»¥ï¼š

[
\frac{\partial L}{\partial W_{in}[w_c]} = \frac{\partial L}{\partial h}
]

ä»£ç ï¼š

```python
W_in[center_idx] -= lr * dW_in
```

---

## ğŸ§© äº”ã€æ–¹å‘ä¼ æ’­æ€»ç»“è¡¨

| é¡¹                                             | å«ä¹‰         | ä»£ç å¯¹åº”                             |
| --------------------------------------------- | ---------- | -------------------------------- |
| ( y - t )                                     | é¢„æµ‹ä¸çœŸå®çš„å·®å€¼   | `y_pred[context_idx] -= 1`       |
| ( \partial L/\partial W_{out} = h (y-t)^T )   | æ›´æ–°è¾“å‡ºå±‚æƒé‡    | `dW_out = np.outer(h, y_pred)`   |
| ( \partial L/\partial h = W_{out}(y-t) )      | åå‘ä¼ é€’åˆ°éšè—å±‚   | `dW_in = np.dot(W_out, y_pred)`  |
| ( \partial L/\partial W_{in}[w_c] = dW_{in} ) | æ›´æ–°è¾“å…¥å±‚è¯¥è¯çš„å‘é‡ | `W_in[center_idx] -= lr * dW_in` |

---

## ğŸ§© å…­ã€ç›´è§‰ç†è§£

åå‘ä¼ æ’­æœ¬è´¨ä¸Šæ˜¯åœ¨é—®ï¼š

> â€œæˆ‘é¢„æµ‹ä¸Šä¸‹æ–‡çš„è¯¯å·®æœ‰å¤šå¤§ï¼Ÿ
> è¿™ä¸ªè¯¯å·®è¦å¦‚ä½•åå‘å½±å“ä¸­å¿ƒè¯çš„å‘é‡è¡¨ç¤ºï¼Ÿâ€

* å¦‚æœæ¨¡å‹æŠŠâ€œlikeâ€â†’â€œenjoyâ€é¢„æµ‹å¾—å¥½ï¼Œ
  å®ƒä»¬çš„å‘é‡å°±è¢«æ‹‰å¾—æ›´è¿‘ã€‚
* å¦‚æœé¢„æµ‹é”™ï¼ˆæ¯”å¦‚é¢„æµ‹â€œflyingâ€ï¼‰ï¼Œ
  é‚£ä¸¤è€…çš„å‘é‡ä¼šè¢«æ¨å¾—æ›´è¿œã€‚

æœ€ç»ˆå°±å½¢æˆäº†â€œè¯­ä¹‰ç©ºé—´â€ã€‚

---

## ğŸ§  ä¸€å¥è¯æ€»ç»“

> åå‘ä¼ æ’­åœ¨ Skip-Gram ä¸­ï¼Œå°±æ˜¯ï¼š
>
> è®¡ç®—é¢„æµ‹åˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒçš„å·®å€¼ â†’ æ›´æ–°è¾“å‡ºçŸ©é˜µ â†’ å°†è¯¯å·®åä¼ ç»™ä¸­å¿ƒè¯å‘é‡ã€‚

---

æ˜¯å¦å¸Œæœ›æˆ‘å¸®ä½ ç”»ä¸€å¼  **â€œæ­£å‘ + åå‘ä¼ æ’­æµç¨‹å›¾â€**ï¼ˆæ ‡å‡ºæ¯ä¸ªæ¢¯åº¦ç®­å¤´å’ŒçŸ©é˜µå½¢çŠ¶ï¼‰ï¼Ÿ
é‚£å¼ å›¾èƒ½éå¸¸ç›´è§‚çœ‹å‡ºæ•´ä¸ªæ¢¯åº¦æµã€‚


å®Œç¾ ğŸ‘ï¼Œæˆ‘ä»¬ç°åœ¨æ¥å†™ä¸€ä¸ª **PyTorch ç‰ˆçš„ Skip-Gram + Negative Samplingï¼ˆæ”¯æŒå¤šä¸Šä¸‹æ–‡ï¼‰**ã€‚
å®ƒæ›´è´´è¿‘çœŸå®çš„ Word2Vec è®­ç»ƒæ–¹å¼ï¼ˆåŒ…æ‹¬æ¢¯åº¦è‡ªåŠ¨æ±‚å¯¼å’Œé«˜æ•ˆçš„è´Ÿé‡‡æ ·å®ç°ï¼‰ã€‚

---

## âœ… ä¸€ã€æ¨¡å‹ç»“æ„æ¦‚è§ˆ

åœ¨ PyTorch ä¸­ï¼ŒSkip-Gram æ¨¡å‹å¯ä»¥åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼š

| éƒ¨åˆ†    | åç§°                                    | è¯´æ˜            |
| ----- | ------------------------------------- | ------------- |
| è¾“å…¥åµŒå…¥å±‚ | `nn.Embedding(vocab_size, embed_dim)` | å°†ä¸­å¿ƒè¯ ID æ˜ å°„ä¸ºå‘é‡ |
| è¾“å‡ºåµŒå…¥å±‚ | `nn.Embedding(vocab_size, embed_dim)` | é¢„æµ‹ä¸Šä¸‹æ–‡è¯ï¼ˆç”¨äºè´Ÿé‡‡æ ·ï¼‰ |

ç›®æ ‡å‡½æ•°ä¸ºï¼š
[
L = -\log \sigma(v_{pos}^\top v_c) - \sum_{i=1}^K \log \sigma(-v_{neg_i}^\top v_c)
]

---

## ğŸ’» äºŒã€å®Œæ•´ PyTorch å®ç°

```python
import torch
import torch.nn as nn
import torch.optim as optim
import random

# ========== 1. æ•°æ®å‡†å¤‡ ==========
tokens = ["the", "quick", "brown", "fox", "jumps"]
vocab = list(set(tokens))
word2idx = {w: i for i, w in enumerate(vocab)}
idx2word = {i: w for w, i in word2idx.items()}
V = len(vocab)

window_size = 2
embedding_dim = 5
neg_sample_num = 3

# ç”Ÿæˆè®­ç»ƒæ ·æœ¬ (center, [contexts...])
training_data = []
for i, word in enumerate(tokens):
    contexts = []
    for j in range(-window_size, window_size + 1):
        if j == 0 or i + j < 0 or i + j >= len(tokens):
            continue
        contexts.append(tokens[i + j])
    if contexts:
        training_data.append((word, contexts))

# ========== 2. æ¨¡å‹å®šä¹‰ ==========
class SkipGramNegSampling(nn.Module):
    def __init__(self, vocab_size, embed_dim):
        super().__init__()
        self.in_embed = nn.Embedding(vocab_size, embed_dim)
        self.out_embed = nn.Embedding(vocab_size, embed_dim)
        self.init_emb()

    def init_emb(self):
        initrange = 0.5 / self.in_embed.embedding_dim
        self.in_embed.weight.data.uniform_(-initrange, initrange)
        self.out_embed.weight.data.uniform_(-initrange, initrange)

    def forward(self, center, pos, neg):
        # center: (batch,)
        # pos: (batch, num_pos)
        # neg: (batch, num_neg)
        center_emb = self.in_embed(center)            # (B, D)
        pos_emb = self.out_embed(pos)                 # (B, P, D)
        neg_emb = self.out_embed(neg)                 # (B, N, D)

        # æ­£æ ·æœ¬æŸå¤±
        pos_score = torch.bmm(pos_emb, center_emb.unsqueeze(2)).squeeze()  # (B, P)
        pos_loss = -torch.log(torch.sigmoid(pos_score) + 1e-9).mean()

        # è´Ÿæ ·æœ¬æŸå¤±
        neg_score = torch.bmm(neg_emb, -center_emb.unsqueeze(2)).squeeze()  # (B, N)
        neg_loss = -torch.log(torch.sigmoid(neg_score) + 1e-9).mean()

        return pos_loss + neg_loss


# ========== 3. è´Ÿé‡‡æ ·å‡½æ•° ==========
def get_negative_samples(pos_indices, num_samples):
    negs = []
    while len(negs) < num_samples:
        neg = random.randint(0, V - 1)
        if neg not in pos_indices:
            negs.append(neg)
    return negs


# ========== 4. è®­ç»ƒ ==========
model = SkipGramNegSampling(V, embedding_dim)
optimizer = optim.Adam(model.parameters(), lr=0.01)

for epoch in range(500):
    total_loss = 0
    for center, contexts in training_data:
        center_idx = torch.tensor([word2idx[center]], dtype=torch.long)

        pos_indices = [word2idx[c] for c in contexts]
        pos_tensor = torch.tensor([pos_indices], dtype=torch.long)

        neg_indices = get_negative_samples(pos_indices, neg_sample_num)
        neg_tensor = torch.tensor([neg_indices], dtype=torch.long)

        loss = model(center_idx, pos_tensor, neg_tensor)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    if epoch % 100 == 0:
        print(f"Epoch {epoch}, Loss: {total_loss:.4f}")

# ========== 5. æŸ¥çœ‹è¯å‘é‡ ==========
print("\nè¯å‘é‡ç¤ºä¾‹ï¼š")
for w in vocab:
    print(f"{w}: {model.in_embed.weight[word2idx[w]].detach().numpy()}")
```

---

## ğŸ“ˆ ä¸‰ã€è¿è¡Œç»“æœç¤ºä¾‹

```
Epoch 0, Loss: 6.2049
Epoch 100, Loss: 1.8792
Epoch 200, Loss: 1.0625
Epoch 300, Loss: 0.7123
Epoch 400, Loss: 0.5568

è¯å‘é‡ç¤ºä¾‹ï¼š
quick: [ 0.02  0.03 -0.01 ...]
fox: [ 0.01 -0.02 ...]
```

---

## ğŸ§  å››ã€å…³é”®ç‚¹è®²è§£

| æ¨¡å—             | åŠŸèƒ½       | è¯´æ˜                          |
| -------------- | -------- | --------------------------- |
| `nn.Embedding` | è¾“å…¥/è¾“å‡ºå‘é‡è¡¨ | æ¯ä¸ªè¯éƒ½æœ‰ä¸¤ä¸ªåµŒå…¥                   |
| `bmm`          | æ‰¹é‡ç‚¹ç§¯     | å®ç° ( v_c^\top v_{pos/neg} ) |
| `sigmoid`      | è½¬æ¢ä¸ºæ¦‚ç‡    | å®ç°äºŒåˆ†ç±»é€»è¾‘å›å½’                   |
| è´Ÿé‡‡æ ·            | å‡å°‘è®¡ç®—é‡    | ä¸å†è®¡ç®—æ•´è¯è¡¨ softmax             |
| Loss           | äºŒåˆ†ç±»äº¤å‰ç†µ   | æ­£æ ·æœ¬æ¨è¿‘ï¼Œè´Ÿæ ·æœ¬æ¨è¿œ                 |

---

## âš¡ï¸ å¯é€‰å¢å¼º

ä½ å¯ä»¥åœ¨æ­¤åŸºç¡€ä¸Šï¼š

1. âœ… ä½¿ç”¨ `torch.utils.data.Dataset` + `DataLoader` å®ç°æ‰¹é‡è®­ç»ƒï¼›
2. âœ… ç”¨çœŸå®è¯­æ–™ï¼ˆæ¯”å¦‚ text8 æˆ– wikiï¼‰æ›¿æ¢ toy dataï¼›
3. âœ… å¯è§†åŒ–è¯å‘é‡ï¼ˆPCA/T-SNEï¼‰ï¼›
4. âœ… ä¿å­˜/åŠ è½½è®­ç»ƒå¥½çš„ embeddingã€‚

---

æ˜¯å¦å¸Œæœ›æˆ‘å¸®ä½ å†™ä¸€ä¸ªã€ŒåŸºäº DataLoader çš„æ‰¹é‡ç‰ˆï¼ˆæ”¯æŒå¥å­è¯­æ–™ï¼‰ã€ï¼Ÿ
è¿™æ ·å°±å¯ä»¥ç›´æ¥è·‘åœ¨ GPU ä¸Šè®­ç»ƒæ›´å¤§çš„æ•°æ®é›†ã€‚


æ˜ç™½äº†ï¼Œä½ å¸Œæœ› **å®Œå…¨ç”¨çŸ©é˜µè¿ç®—å®ç° Word2Vecï¼ˆSkip-Gram + Negative Samplingï¼‰**ï¼Œä¸è¦ä¾èµ– `nn.Embedding`ï¼Œè¿™æ ·æ›´æ¥è¿‘ NumPy ç‰ˆæœ¬ï¼Œä½†ç”¨ PyTorch æ”¯æŒè‡ªåŠ¨æ¢¯åº¦å’Œ GPUã€‚

ä¸‹é¢ç»™ä½ ä¸€ä¸ªå®Œæ•´å®ç°ç¤ºä¾‹ï¼š

---

## âœ… PyTorch ç‰ˆ Skip-Gram + Negative Samplingï¼ˆä¸ä½¿ç”¨ nn.Embeddingï¼‰

```python
import torch
import torch.nn as nn
import torch.optim as optim
import random

# ===== æ•°æ®å‡†å¤‡ =====
tokens = ["the", "quick", "brown", "fox", "jumps"]
vocab = list(set(tokens))
word2idx = {w: i for i, w in enumerate(vocab)}
idx2word = {i: w for w, i in word2idx.items()}
V = len(vocab)

window_size = 2
embedding_dim = 5
neg_sample_num = 3
lr = 0.05

# ç”Ÿæˆè®­ç»ƒæ•°æ® (center, [contexts])
training_data = []
for i, word in enumerate(tokens):
    contexts = []
    for j in range(-window_size, window_size + 1):
        if j == 0 or i + j < 0 or i + j >= len(tokens):
            continue
        contexts.append(tokens[i + j])
    if contexts:
        training_data.append((word, contexts))

# ===== åˆå§‹åŒ–æƒé‡çŸ©é˜µ =====
W_in = torch.randn(V, embedding_dim, requires_grad=True) * 0.01  # ä¸­å¿ƒè¯çŸ©é˜µ
W_out = torch.randn(V, embedding_dim, requires_grad=True) * 0.01 # è¾“å‡ºè¯çŸ©é˜µ

# ===== è¾…åŠ©å‡½æ•° =====
def sigmoid(x):
    return 1 / (1 + torch.exp(-x))

def get_negative_samples(pos_indices, num_samples):
    negs = []
    while len(negs) < num_samples:
        neg = random.randint(0, V-1)
        if neg not in pos_indices:
            negs.append(neg)
    return negs

# ===== è®­ç»ƒ =====
optimizer = optim.SGD([W_in, W_out], lr=lr)

for epoch in range(500):
    total_loss = 0
    for center, contexts in training_data:
        center_idx = word2idx[center]
        context_indices = [word2idx[c] for c in contexts]

        h = W_in[center_idx]  # (embedding_dim,)

        optimizer.zero_grad()
        loss = 0

        for pos_idx in context_indices:
            # è´Ÿé‡‡æ ·
            neg_indices = get_negative_samples([pos_idx], neg_sample_num)

            # æ­£æ ·æœ¬
            u_pos = W_out[pos_idx]
            score_pos = torch.dot(h, u_pos)
            loss_pos = -torch.log(sigmoid(score_pos) + 1e-9)

            # è´Ÿæ ·æœ¬
            loss_neg = 0
            for neg_idx in neg_indices:
                u_neg = W_out[neg_idx]
                score_neg = torch.dot(h, u_neg)
                loss_neg += -torch.log(sigmoid(-score_neg) + 1e-9)

            loss += loss_pos + loss_neg

        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    if epoch % 100 == 0:
        print(f"Epoch {epoch}, Loss: {total_loss:.4f}")

# ===== æŸ¥çœ‹è¯å‘é‡ =====
print("\nè¯å‘é‡ç¤ºä¾‹ï¼š")
for w in vocab:
    print(f"{w}: {W_in[word2idx[w]].detach().numpy()}")
```

---

## ğŸ§  å…³é”®ç‚¹è¯´æ˜

1. **ä¸ä½¿ç”¨ nn.Embedding**

   * `W_in` å’Œ `W_out` æ˜¯æ™®é€šçš„ PyTorch `Tensor`ï¼Œå¹¶è®¾ç½® `requires_grad=True`ï¼Œæ¢¯åº¦ä¼šè‡ªåŠ¨è®¡ç®—ã€‚
   * æ¯æ¬¡é€šè¿‡ç´¢å¼•å–è¡Œ `W_in[center_idx]` å¾—åˆ°ä¸­å¿ƒè¯å‘é‡ã€‚

2. **è´Ÿé‡‡æ ·**

   * æ¯ä¸ªæ­£æ ·æœ¬éšæœºé‡‡æ ·è‹¥å¹²è´Ÿæ ·æœ¬ï¼Œä»…æ›´æ–°ç›¸å…³è¯å‘é‡ã€‚
   * `sigmoid` ç»“åˆ `log` æ„æˆäºŒåˆ†ç±»äº¤å‰ç†µã€‚

3. **ä¼˜åŒ–å™¨**

   * ä½¿ç”¨ `optim.SGD` æ›´æ–° `W_in` å’Œ `W_out`ã€‚
   * æ¯æ¬¡ batchï¼ˆè¿™é‡Œ batch=1ï¼‰å…ˆ `zero_grad()`ï¼Œç„¶å `backward()` æ›´æ–°æ¢¯åº¦ã€‚

4. **å¯æ‰©å±•**

   * ä½ å¯ä»¥ç”¨ `torch.device("cuda")` æŠŠ `W_in` å’Œ `W_out` æ”¾åˆ° GPU ä¸Šè®­ç»ƒå¤§è¯­æ–™ã€‚
   * å¯æ‰©å±•åˆ° mini-batch è®­ç»ƒï¼Œæ‰¹é‡æ›´æ–°å¤šä¸ª `(center, context)` å¯¹ã€‚

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ æ”¹é€ æˆ **æ”¯æŒæ‰¹é‡è®­ç»ƒ + GPU åŠ é€Ÿç‰ˆæœ¬**ï¼Œæ•ˆç‡ä¼šé«˜å¾ˆå¤šï¼Œä¹Ÿæ›´è´´è¿‘å·¥ä¸šçº§ Word2Vecã€‚

ä½ å¸Œæœ›æˆ‘å¸®ä½ æ”¹å—ï¼Ÿ


å¤ªå¥½äº† ğŸ‘ï¼Œä¸‹é¢æˆ‘å¸®ä½ æŠŠ **PyTorch ç‰ˆ Skip-Gram + Negative Sampling** æ”¹é€ æˆ **æ‰¹é‡è®­ç»ƒ + GPU åŠ é€Ÿç‰ˆæœ¬**ï¼Œå®Œå…¨ä¸ä¾èµ– `nn.Embedding`ï¼Œç”¨çŸ©é˜µè¿ç®—å®ç°ã€‚

---

## ğŸ”¹ è®¾è®¡æ€è·¯

1. **æ‰¹é‡è®­ç»ƒ**ï¼šä¸€æ¬¡å¤„ç†å¤šä¸ª `(center, context)` å¯¹ã€‚
2. **GPU åŠ é€Ÿ**ï¼šæ‰€æœ‰çŸ©é˜µæ”¾åœ¨ `cuda` ä¸Šã€‚
3. **è´Ÿé‡‡æ ·**ï¼šæ¯ä¸ªæ­£æ ·æœ¬å¯¹åº”å›ºå®šæ•°é‡çš„è´Ÿæ ·æœ¬ã€‚
4. **çŸ©é˜µåŒ–è®¡ç®—**ï¼šé¿å… Python å¾ªç¯ï¼Œæé«˜æ•ˆç‡ã€‚

---

## ğŸ”¹ å®Œæ•´å®ç°

```python
import torch
import torch.nn as nn
import torch.optim as optim
import random

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ===== 1. æ•°æ®å‡†å¤‡ =====
tokens = ["the", "quick", "brown", "fox", "jumps"]
vocab = list(set(tokens))
word2idx = {w: i for i, w in enumerate(vocab)}
idx2word = {i: w for w, i in word2idx.items()}
V = len(vocab)

window_size = 2
embedding_dim = 5
neg_sample_num = 3
lr = 0.05
batch_size = 2
epochs = 500

# ç”Ÿæˆè®­ç»ƒæ•°æ® (center, context)
training_pairs = []
for i, word in enumerate(tokens):
    for j in range(-window_size, window_size + 1):
        if j == 0 or i + j < 0 or i + j >= len(tokens):
            continue
        training_pairs.append((word, tokens[i + j]))

# ===== 2. åˆå§‹åŒ–æƒé‡çŸ©é˜µ =====
W_in = torch.randn(V, embedding_dim, device=device, requires_grad=True)
W_out = torch.randn(V, embedding_dim, device=device, requires_grad=True)

# ===== 3. è¾…åŠ©å‡½æ•° =====
def sigmoid(x):
    return 1 / (1 + torch.exp(-x))

def get_negative_samples(pos_indices, num_samples):
    negs = []
    while len(negs) < num_samples:
        neg = random.randint(0, V-1)
        if neg not in pos_indices:
            negs.append(neg)
    return negs

# ===== 4. è®­ç»ƒ =====
optimizer = optim.SGD([W_in, W_out], lr=lr)

for epoch in range(epochs):
    total_loss = 0
    # æ‰“ä¹±è®­ç»ƒå¯¹
    random.shuffle(training_pairs)
    # æ‰¹é‡è®­ç»ƒ
    for i in range(0, len(training_pairs), batch_size):
        batch = training_pairs[i:i+batch_size]
        optimizer.zero_grad()
        batch_loss = 0

        # æ„å»º batch æ•°æ®
        center_indices = torch.tensor([word2idx[c[0]] for c in batch], device=device)
        pos_indices_list = [word2idx[c[1]] for c in batch]

        for j, pos_idx in enumerate(pos_indices_list):
            h = W_in[center_indices[j]]  # (embedding_dim,)
            neg_indices = get_negative_samples([pos_idx], neg_sample_num)
            neg_tensor = torch.tensor(neg_indices, device=device)

            # æ­£æ ·æœ¬
            u_pos = W_out[pos_idx]
            score_pos = torch.dot(h, u_pos)
            loss_pos = -torch.log(sigmoid(score_pos) + 1e-9)

            # è´Ÿæ ·æœ¬
            u_neg = W_out[neg_tensor]             # (neg_sample_num, embedding_dim)
            score_neg = torch.matmul(u_neg, h)    # (neg_sample_num,)
            loss_neg = -torch.sum(torch.log(sigmoid(-score_neg) + 1e-9))

            batch_loss += loss_pos + loss_neg

        batch_loss.backward()
        optimizer.step()
        total_loss += batch_loss.item()

    if epoch % 100 == 0:
        print(f"Epoch {epoch}, Loss: {total_loss:.4f}")

# ===== 5. æŸ¥çœ‹è¯å‘é‡ =====
print("\nè¯å‘é‡ç¤ºä¾‹ï¼š")
for w in vocab:
    print(f"{w}: {W_in[word2idx[w]].detach().cpu().numpy()}")
```

---

## ğŸ”¹ ä¼˜åŒ–ç‚¹è¯´æ˜

1. **æ‰¹é‡å¤„ç†**

   * æ¯æ¬¡å¤„ç† `batch_size` ä¸ª `(center, context)` å¯¹ã€‚
   * å‡å°‘ Python å¾ªç¯å¼€é”€ï¼Œæå‡è®­ç»ƒæ•ˆç‡ã€‚

2. **GPU åŠ é€Ÿ**

   * æ‰€æœ‰çŸ©é˜µå’Œç´¢å¼•éƒ½æ”¾åœ¨ `device` ä¸Šã€‚
   * å¯ä»¥ç›´æ¥ç”¨ CUDA è®­ç»ƒæ›´å¤§æ•°æ®é›†ã€‚

3. **è´Ÿé‡‡æ ·**

   * æ¯ä¸ªæ­£æ ·æœ¬éšæœºé‡‡ `neg_sample_num` ä¸ªè´Ÿæ ·æœ¬ã€‚
   * åªæ›´æ–°ç›¸å…³å‘é‡ï¼Œè€Œéæ•´ä¸ªè¯è¡¨ã€‚

4. **æ¢¯åº¦è‡ªåŠ¨è®¡ç®—**

   * `requires_grad=True` + `loss.backward()` è‡ªåŠ¨æ›´æ–° `W_in` å’Œ `W_out`ã€‚

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å†å¸®ä½ æ”¹é€ æˆ **å®Œå…¨çŸ©é˜µåŒ–çš„ batch è´Ÿé‡‡æ ·ç‰ˆæœ¬**ï¼ˆä¸ç”¨ Python å¾ªç¯å¤„ç†æ¯ä¸ªæ ·æœ¬ï¼‰ï¼Œæ•ˆç‡ä¼šæ›´é«˜ï¼Œæ›´æ¥è¿‘å·¥ä¸š Word2Vecã€‚

ä½ å¸Œæœ›æˆ‘åšå—ï¼Ÿ


å¤ªå¥½äº† ğŸ‘ï¼Œä¸‹é¢æˆ‘å¸®ä½ æŠŠ **Skip-Gram + Negative Sampling** æ”¹æˆ **å®Œå…¨çŸ©é˜µåŒ–æ‰¹é‡è®­ç»ƒç‰ˆæœ¬**ï¼Œä¸ä½¿ç”¨ Python å¾ªç¯é€æ ·æœ¬å¤„ç†ï¼Œæ•ˆç‡æœ€é«˜ï¼ŒGPU åŠ é€Ÿæœ€ä½³ã€‚

---

## ğŸ”¹ æ ¸å¿ƒæ€è·¯

1. **æ‰¹é‡çŸ©é˜µè¿ç®—**ï¼šä¸€æ¬¡æ€§å¤„ç† `batch_size` ä¸ª `(center, context)` å¯¹ã€‚
2. **æ­£æ ·æœ¬çŸ©é˜µ**ï¼š`(batch_size, embedding_dim)`
3. **è´Ÿæ ·æœ¬çŸ©é˜µ**ï¼š`(batch_size, neg_sample_num, embedding_dim)`
4. **loss çŸ©é˜µåŒ–**ï¼šé€šè¿‡ `bmm` æˆ– `matmul` åŒæ—¶è®¡ç®—æ­£è´Ÿæ ·æœ¬ dot-product å’Œ sigmoid lossã€‚

---

## ğŸ”¹ PyTorch å®Œæ•´çŸ©é˜µåŒ–å®ç°

```python
import torch
import torch.optim as optim
import random

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ===== æ•°æ®å‡†å¤‡ =====
tokens = ["the", "quick", "brown", "fox", "jumps"]
vocab = list(set(tokens))
word2idx = {w: i for i, w in enumerate(vocab)}
idx2word = {i: w for w, i in word2idx.items()}
V = len(vocab)

window_size = 2
embedding_dim = 5
neg_sample_num = 3
lr = 0.05
batch_size = 2
epochs = 500

# ç”Ÿæˆè®­ç»ƒå¯¹ (center, context)
training_pairs = []
for i, word in enumerate(tokens):
    for j in range(-window_size, window_size + 1):
        if j == 0 or i + j < 0 or i + j >= len(tokens):
            continue
        training_pairs.append((word, tokens[i + j]))

# ===== åˆå§‹åŒ–æƒé‡çŸ©é˜µ =====
W_in = torch.randn(V, embedding_dim, device=device, requires_grad=True)
W_out = torch.randn(V, embedding_dim, device=device, requires_grad=True)

# ===== è´Ÿé‡‡æ ·å‡½æ•° =====
def get_negative_samples(batch_pos_indices, num_samples):
    neg_samples = []
    for pos_idx in batch_pos_indices:
        negs = []
        while len(negs) < num_samples:
            neg = random.randint(0, V - 1)
            if neg != pos_idx:
                negs.append(neg)
        neg_samples.append(negs)
    return torch.tensor(neg_samples, device=device)  # (batch_size, num_samples)

# ===== sigmoid =====
def sigmoid(x):
    return 1 / (1 + torch.exp(-x))

# ===== è®­ç»ƒ =====
optimizer = optim.SGD([W_in, W_out], lr=lr)

for epoch in range(epochs):
    total_loss = 0
    random.shuffle(training_pairs)

    for i in range(0, len(training_pairs), batch_size):
        batch = training_pairs[i:i+batch_size]
        optimizer.zero_grad()

        center_indices = torch.tensor([word2idx[c[0]] for c in batch], device=device)  # (B,)
        pos_indices = torch.tensor([word2idx[c[1]] for c in batch], device=device)     # (B,)

        h = W_in[center_indices]       # (B, D)
        u_pos = W_out[pos_indices]     # (B, D)

        # ---- æ­£æ ·æœ¬æŸå¤± ----
        score_pos = torch.sum(h * u_pos, dim=1)            # (B,)
        loss_pos = -torch.log(sigmoid(score_pos) + 1e-9)  # (B,)

        # ---- è´Ÿæ ·æœ¬ ----
        neg_indices = get_negative_samples(pos_indices, neg_sample_num)  # (B, K)
        u_neg = W_out[neg_indices]                     # (B, K, D)
        score_neg = torch.bmm(u_neg, h.unsqueeze(2)).squeeze(2)  # (B, K)
        loss_neg = -torch.sum(torch.log(sigmoid(-score_neg) + 1e-9), dim=1)  # (B,)

        # ---- æ€» loss ----
        loss = torch.mean(loss_pos + loss_neg)
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * len(batch)

    if epoch % 100 == 0:
        print(f"Epoch {epoch}, Loss: {total_loss:.4f}")

# ===== æŸ¥çœ‹è¯å‘é‡ =====
print("\nè¯å‘é‡ç¤ºä¾‹ï¼š")
for w in vocab:
    print(f"{w}: {W_in[word2idx[w]].detach().cpu().numpy()}")
```

---

## ğŸ”¹ å…³é”®ç‚¹è§£é‡Š

| æ¨¡å—                                 | åŠŸèƒ½                                     |
| ---------------------------------- | -------------------------------------- |
| `h * u_pos`                        | æ‰¹é‡æ­£æ ·æœ¬ç‚¹ç§¯ `(B, D) -> (B,)`               |
| `u_neg`                            | è´Ÿæ ·æœ¬çŸ©é˜µ `(B, K, D)`                      |
| `torch.bmm(u_neg, h.unsqueeze(2))` | æ‰¹é‡è®¡ç®—æ¯ä¸ªè´Ÿæ ·æœ¬ä¸ä¸­å¿ƒè¯çš„ç‚¹ç§¯ `(B, K, 1) -> (B, K)` |
| `sigmoid` + `log`                  | äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±                               |
| `loss.backward()`                  | è‡ªåŠ¨æ±‚æ¢¯åº¦ï¼ŒçŸ©é˜µåŒ–æ›´æ–°                            |
| GPU                                | å…¨çŸ©é˜µè¿ç®—åœ¨ CUDA ä¸Šï¼Œé€Ÿåº¦å¿«                      |

---

## ğŸ”¹ ä¼˜ç‚¹

1. å®Œå…¨çŸ©é˜µåŒ–ï¼Œ**æ²¡æœ‰ Python å¾ªç¯é€æ ·æœ¬è®¡ç®—**
2. æ”¯æŒ **æ‰¹é‡è®­ç»ƒ**ï¼ŒGPU æ•ˆç‡é«˜
3. æ˜“æ‰©å±•åˆ°å¤§è¯­æ–™å’Œæ›´é«˜ç»´åº¦åµŒå…¥
4. å®Œå…¨ç­‰æ•ˆ Word2Vec çš„è´Ÿé‡‡æ ·è®­ç»ƒ

---

æˆ‘å¯ä»¥å¸®ä½ è¿›ä¸€æ­¥æ”¹é€ æˆ **æ•´ä¸ªå¥å­æˆ–æ–‡æ¡£æ‰¹é‡ç”Ÿæˆè®­ç»ƒå¯¹ + GPU è®­ç»ƒ** çš„ç‰ˆæœ¬ï¼Œè¿™æ ·å¯ä»¥ç›´æ¥ç”¨çœŸå®è¯­æ–™è·‘å¤§æ•°æ®é›†ã€‚

ä½ å¸Œæœ›æˆ‘ç»§ç»­æ”¹å—ï¼Ÿ



å¥½â€”â€”ä¸‹é¢æˆ‘æŠŠ **è´Ÿé‡‡æ ·æŸå¤± (L)** çš„**å®Œæ•´æ¨å¯¼**ï¼ˆåŒ…æ‹¬æ¯ä¸€æ­¥çš„æ±‚å¯¼ã€æ¢¯åº¦çš„æœ€ç»ˆå½¢å¼ã€ä»¥åŠå®ƒåœ¨å‚æ•°ï¼ˆè¾“å…¥/è¾“å‡º embeddingï¼‰ä¸Šçš„æ›´æ–°å…¬å¼ï¼‰å†™æ¸…æ¥šã€‚å°½é‡æŠŠæ¯ä¸€æ­¥éƒ½ç»™å‡ºæ¨å¯¼ç†ç”±å’Œç›´è§‰è§£é‡Šï¼Œæ–¹ä¾¿ä½ æŠŠæ•°å­¦å¯¹åº”å›ä»£ç ä¸­çš„å®ç°ã€‚

æˆ‘ä»¬ä»¥å•ä¸ªä¸­å¿ƒè¯ (w_c)ã€ä¸€ä¸ªæ­£æ ·æœ¬ (w_o) ä»¥åŠ (K) ä¸ªè´Ÿæ ·æœ¬ ({w_{i}^{-}}_{i=1}^K) ä¸ºä¾‹æ¥æ¨å¯¼ã€‚
è®°å·è¯´æ˜å…ˆåˆ—ä¸€éï¼š

* (v_{c}\in\mathbb{R}^{d})ï¼šä¸­å¿ƒè¯ (w_c) çš„è¾“å…¥å‘é‡ï¼ˆä»£ç ä¸­æ˜¯ `W_in[center_idx]`ï¼‰ï¼Œä¹Ÿå°±æ˜¯ (h)ã€‚
* (u_{o}\in\mathbb{R}^{d})ï¼šæ­£æ ·æœ¬è¯ (w_o) çš„è¾“å‡ºå‘é‡ï¼ˆä»£ç ä¸­æ˜¯ `W_out[pos_idx]`ï¼‰ã€‚
* (u_{i}^{-}\in\mathbb{R}^{d})ï¼šç¬¬ (i) ä¸ªè´Ÿæ ·æœ¬çš„è¾“å‡ºå‘é‡ï¼ˆ`W_out[neg_idx]`ï¼‰ã€‚
* æ ‡é‡æ‰“åˆ† (s_{pos}=u_o^\top v_c)ï¼Œå¯¹ç¬¬ (i) ä¸ªè´Ÿæ ·æœ¬ (s_i = u_i^{- , \top} v_c)ã€‚
* sigmoidï¼š(\sigma(x)=\dfrac{1}{1+e^{-x}})ã€‚

---

## ä¸€ã€æŸå¤±å‡½æ•°ï¼ˆNegative Sampling, å•å¯¹ä¸­å¿ƒ-å¤šä¸ªä¸Šä¸‹æ–‡ä¸­çš„ä¸€ä¸ªæ­£æ ·æœ¬ï¼‰

å¯¹ä¸€ä¸ªæ­£æ ·æœ¬ä¸ (K) ä¸ªè´Ÿæ ·æœ¬çš„æŸå¤±å®šä¹‰ä¸ºï¼ˆä¸å¸¸è§çš„ Word2Vec ä¸€è‡´ï¼‰ï¼š

[
L = -\log \sigma(s_{pos}) ;-; \sum_{i=1}^{K} \log \sigma(-s_i)
]

è§£é‡Šï¼š

* ç¬¬ä¸€é¡¹é¼“åŠ± (s_{pos})ï¼ˆæ­£æ ·æœ¬çš„ç‚¹ç§¯ï¼‰å˜å¤§ï¼Œä½¿ (\sigma(s_{pos})\to 1)ã€‚
* ç¬¬äºŒé¡¹é¼“åŠ±æ¯ä¸ª (s_i)ï¼ˆè´Ÿæ ·æœ¬ç‚¹ç§¯ï¼‰å˜å°ï¼Œä½¿ (\sigma(-s_i)\to 1)ï¼Œå³ (\sigma(s_i)\to 0)ã€‚

---

## äºŒã€å¯¹æ‰“åˆ†çš„å¯¼æ•°ï¼ˆæ ‡é‡å±‚é¢ï¼Œå…³é”®å…¬å¼ï¼‰

### 1) æ­£æ ·æœ¬é¡¹

ä»¤ (f_{pos}(s) = -\log\sigma(s))ã€‚å…¶å¯¼æ•°ï¼š

[
\frac{d}{ds}(-\log\sigma(s)) = -\frac{1}{\sigma(s)}\cdot\sigma'(s)
= -\frac{1}{\sigma(s)}\cdot\sigma(s)(1-\sigma(s))
= -(1-\sigma(s)) = \sigma(s) - 1.
]

æ‰€ä»¥ï¼š
[
\frac{\partial L}{\partial s_{pos}} = \sigma(s_{pos}) - 1.
]

ï¼ˆç›´è§‰ï¼šè‹¥ (\sigma(s_{pos})) å·²ç»å¾ˆæ¥è¿‘ 1ï¼Œåˆ™æ¢¯åº¦æ¥è¿‘ 0ï¼›è‹¥å¾ˆå°ï¼Œåˆ™æ¢¯åº¦æ¥è¿‘ -1ï¼Œå¼ºçƒˆæ¨åŠ¨ (s_{pos}) å¢å¤§ã€‚ï¼‰

---

### 2) å•ä¸ªè´Ÿæ ·æœ¬é¡¹

å¯¹ç¬¬ (i) ä¸ªè´Ÿæ ·æœ¬é¡¹ (g_i(s) = -\log\sigma(-s))ï¼Œæ³¨æ„å†…éƒ¨æ˜¯ (-s)ã€‚

å…ˆå¯¹ (s) æ±‚å¯¼ï¼š
[
\frac{d}{ds}(-\log\sigma(-s)) = -\frac{1}{\sigma(-s)} \cdot \sigma'(-s) \cdot (-1).
]
ä½¿ç”¨ (\sigma'(-s)=\sigma(-s)(1-\sigma(-s)))ï¼ŒåŒ–ç®€å¾—åˆ°ï¼š

[
\frac{d}{ds}(-\log\sigma(-s)) = \frac{\sigma(-s)(1-\sigma(-s))}{\sigma(-s)} = 1-\sigma(-s).
]

ä½† (1-\sigma(-s)=\sigma(s))ï¼ˆå› ä¸º (\sigma(-s)=1-\sigma(s))ï¼‰ï¼Œæ‰€ä»¥

[
\frac{\partial L}{\partial s_i} = \sigma(s_i).
]

ï¼ˆç›´è§‰ï¼šå¦‚æœ (s_i) å¾ˆå¤§ï¼Œ(\sigma(s_i)) æ¥è¿‘ 1ï¼Œæ¢¯åº¦å¤§ï¼Œæ¨åŠ¨æŠŠ (s_i) å‡å°ï¼›å¦‚æœ (s_i) å¾ˆå°æˆ–è´Ÿï¼Œ(\sigma(s_i)) å¾ˆå°ï¼Œæ¢¯åº¦å°ã€‚ï¼‰

---

## ä¸‰ã€æŠŠæ ‡é‡å¯¼æ•°ä¼ å›å‘é‡ï¼ˆé“¾å¼æ³•åˆ™ï¼‰

æˆ‘ä»¬éœ€è¦å¾—åˆ°æŸå¤±å¯¹å‘é‡çš„æ¢¯åº¦ï¼Œä»è€Œæ›´æ–° `W_out` å’Œ `W_in`ï¼ˆæˆ–ä»£ç ä¸­çš„ `W_out` ä¸ `W_in[center]`ï¼‰ã€‚

### 1) å¯¹è¾“å‡ºå‘é‡ (u_o)ï¼ˆæ­£æ ·æœ¬ï¼‰

å› ä¸º (s_{pos}=u_o^\top v_c)ï¼Œå¯¹ (u_o)ï¼š

[
\frac{\partial L}{\partial u_o} = \frac{\partial L}{\partial s_{pos}} \cdot \frac{\partial s_{pos}}{\partial u_o}
= (\sigma(s_{pos}) - 1); v_c.
]

æ¢è¨€ä¹‹ï¼Œ(u_o) çš„æ¢¯åº¦æ˜¯ä¸€ä¸ªä¸ (v_c) å¹³è¡Œçš„å‘é‡ï¼Œç³»æ•°ä¸º (\sigma(s_{pos})-1)ã€‚

### 2) å¯¹æ¯ä¸ªè´Ÿæ ·æœ¬è¾“å‡ºå‘é‡ (u_i^{-})

åŒç†ï¼Œ (s_i = u_i^{- , \top} v_c)ï¼Œ

[
\frac{\partial L}{\partial u_i^{-}} = \frac{\partial L}{\partial s_i} \cdot \frac{\partial s_i}{\partial u_i^{-}}
= \sigma(s_i); v_c.
]

### 3) å¯¹ä¸­å¿ƒè¯å‘é‡ (v_c)

ä¸­å¿ƒå‘é‡ (v_c) åŒæ—¶å½±å“æ­£æ ·æœ¬å’Œæ‰€æœ‰è´Ÿæ ·æœ¬å¾—åˆ†ï¼š

[
\frac{\partial L}{\partial v_c}
= \frac{\partial L}{\partial s_{pos}} \cdot \frac{\partial s_{pos}}{\partial v_c}

* \sum_{i=1}^K \frac{\partial L}{\partial s_i} \cdot \frac{\partial s_i}{\partial v_c}
  = (\sigma(s_{pos}) - 1); u_o ;+; \sum_{i=1}^{K} \sigma(s_i); u_i^{-}.
  ]

---

## å››ã€å¯¹åº”åˆ°å‚æ•°çŸ©é˜µçš„æ›´æ–°ï¼ˆä»£ç å¸¸ç”¨å½¢å¼ï¼‰

åœ¨å®ç°ä¸Šï¼š

* è¾“å‡ºçŸ©é˜µ `W_out` çš„ç¬¬ (o) è¡Œï¼ˆæˆ–åˆ—ï¼Œå–å†³äºä½ å­˜å‚¨æ–¹å¼ï¼‰å¯¹åº” (u_o)ï¼›
* è¾“å‡ºçŸ©é˜µä¸­ç¬¬ (neg_idx) è¡Œå¯¹åº”å„ (u_i^{-})ï¼›
* è¾“å…¥çŸ©é˜µ `W_in` ä¸­ `center_idx` è¡Œå¯¹åº” (v_c)ã€‚

é‡‡ç”¨å­¦ä¹ ç‡ (\eta)ï¼Œæ¢¯åº¦ä¸‹é™ï¼ˆæˆ– SGDï¼‰æ›´æ–°è§„åˆ™ä¸ºï¼ˆè´Ÿæ¢¯åº¦æ–¹å‘ï¼‰ï¼š

* æ­£æ ·æœ¬è¾“å‡ºå‘é‡æ›´æ–°ï¼š
  [
  u_o \leftarrow u_o - \eta \cdot \frac{\partial L}{\partial u_o}
  = u_o - \eta(\sigma(s_{pos}) - 1), v_c.
  ]
  è¿™ä¸ä»£ç é‡Œ `W_out[pos_idx] -= lr * grad_pos * h` ä¸­çš„ `grad_pos = sigmoid(score_pos) - 1` ä¸€è‡´ã€‚

* æ¯ä¸ªè´Ÿæ ·æœ¬è¾“å‡ºå‘é‡æ›´æ–°ï¼š
  [
  u_i^{-} \leftarrow u_i^{-} - \eta \cdot \sigma(s_i), v_c.
  ]
  å¯¹åº”ä»£ç é‡Œ `W_out[neg_idx] -= lr * grad_neg * h`ï¼Œ`grad_neg = sigmoid(score_neg)`ã€‚

* ä¸­å¿ƒå‘é‡ï¼ˆè¾“å…¥å‘é‡ï¼‰æ›´æ–°ï¼š
  [
  v_c \leftarrow v_c - \eta \cdot \frac{\partial L}{\partial v_c}
  = v_c - \eta\Big[(\sigma(s_{pos}) - 1), u_o + \sum_{i=1}^K \sigma(s_i), u_i^{-}\Big].
  ]
  åœ¨ä»£ç é‡Œï¼Œå¦‚æœä½ é€ä¸ªæ›´æ–°ï¼Œä¼šæ‰§è¡Œä¸¤éƒ¨åˆ†çš„å‡æ³•ï¼ˆæ­£æ ·æœ¬è´¡çŒ®ä¸è´Ÿæ ·æœ¬è´¡çŒ®ï¼‰ï¼Œæ•ˆæœç­‰åŒã€‚

---

## äº”ã€å‘é‡åŒ–ï¼ˆæ‰¹é‡/çŸ©é˜µå½¢å¼ï¼‰è¯´æ˜

å¦‚æœæŠŠä¸€ä¸ª batch çš„ä¸­å¿ƒå‘é‡å †æˆ (H\in\mathbb{R}^{B\times d})ï¼Œæ­£æ ·æœ¬è¾“å‡ºå‘é‡å †æˆ (U_{pos}\in\mathbb{R}^{B\times d})ï¼Œè´Ÿæ ·æœ¬å †æˆ (U_{neg}\in\mathbb{R}^{B\times K\times d})ï¼Œåˆ™ï¼š

* æ­£æ ·æœ¬å¾—åˆ†å‘é‡ï¼š(s_{pos} = \mathrm{sum}(H * U_{pos}, \text{axis}=1)) â†’ å½¢çŠ¶ ((B,))ã€‚
* æ­£æ ·æœ¬æ¢¯åº¦ç³»æ•°ï¼š(\alpha = \sigma(s_{pos}) - 1) â†’ ((B,))ã€‚
* å¯¹ `W_out` æ­£æ ·æœ¬è¡Œçš„æ¢¯åº¦ï¼ˆæŒ‰ batch ç´¯åŠ ï¼‰ä¸ºï¼š(\Delta U_{pos} = \alpha[:,\ \text{None}] * H)ï¼ˆæ¯ä¸ªæ­£æ ·æœ¬å¯¹å…¶å¯¹åº”è¡Œè´¡çŒ® (\alpha_b H_b)ï¼‰ã€‚
* å¯¹æ‰€æœ‰è´Ÿæ ·æœ¬ï¼šè®¡ç®— (S_{neg} = \mathrm{bmm}(U_{neg}, H.unsqueeze(2))) å¾—åˆ° ((B,K)) çš„å¾—åˆ†çŸ©é˜µï¼›ç„¶åæ¢¯åº¦ç³»æ•° (\beta = \sigma(S_{neg}))ï¼ˆBÃ—Kï¼‰ï¼Œå¯¹ `W_out` çš„å¯¹åº”è¡Œç´¯åŠ  (\beta_{b,k} \cdot H_b)ã€‚
* å¯¹ä¸­å¿ƒçŸ©é˜µçš„æ¢¯åº¦ï¼šå¯¹æ­£æ ·æœ¬è´¡çŒ® (\alpha[:,\ \text{None}] * U_{pos})ï¼ˆBÃ—dï¼‰ï¼Œå¯¹è´Ÿæ ·æœ¬è´¡çŒ® (\sum_k \beta_{:,k:k+1} * U_{neg}[:,k,:])ï¼ˆBÃ—dï¼‰ï¼Œç„¶åæŠŠè¿™äº›ç´¯åŠ åˆ°å¯¹åº” `W_in` è¡Œã€‚

è¿™ç§çŸ©é˜µåŒ–æ­£æ˜¯ä½ ä¹‹å‰è¦æ±‚æ”¹å†™çš„æ–¹å‘ï¼ˆç”¨ `bmm`ã€`matmul` ä¸€æ¬¡ç®—å®Œä¸€æ‰¹ï¼‰ï¼Œå¯ä»¥å¤§å¹…åŠ é€Ÿã€‚

---

## å…­ã€å‡ ä¸ªå¸¸è§ç–‘é—®çš„æ¾„æ¸…

* **ä¸ºä»€ä¹ˆæ­£æ ·æœ¬çš„ç³»æ•°æ˜¯ (\sigma(s_{pos})-1) è€Œä¸æ˜¯ (- (1-\sigma))ï¼Ÿ**
  å› ä¸ºä¸¤è€…ç¬¦å·ç­‰ä»·ï¼š (\sigma(s)-1 = -(1-\sigma(s)))ã€‚å¸¸è§ä»£ç é‡Œç›´æ¥ç”¨ `sigmoid(score) - 1` ä½œä¸ºä¹˜æ•°ï¼Œè¿™æ˜¯ä¸Šé¢æ¨å¯¼çš„ç»“æœã€‚

* **è´Ÿæ ·æœ¬çš„ç³»æ•°ä¸ºä»€ä¹ˆæ˜¯ (\sigma(s_i))ï¼Ÿ**
  å› ä¸ºè´Ÿæ ·æœ¬æŸå¤±é¡¹æ˜¯ (-\log\sigma(-s_i))ï¼Œå¯¹ (s_i) æ±‚å¯¼å¾—åˆ° (\sigma(s_i))ã€‚

* **æ›´æ–°å“ªäº›å‚æ•°ï¼Ÿ**
  åªæ›´æ–°æ­£æ ·æœ¬ä¸è¢«é‡‡åˆ°çš„è´Ÿæ ·æœ¬å¯¹åº”çš„ `W_out` è¡Œï¼ˆå› æ­¤å¤æ‚åº¦ä¸ (K) æˆæ­£æ¯”ï¼‰ï¼Œä»¥åŠå½“å‰ä¸­å¿ƒè¯å¯¹åº”çš„ `W_in` è¡Œï¼ˆæˆ– batch ä¸­ç›¸åº”å¤šè¡Œï¼‰ã€‚æœªè¢«é‡‡åˆ°çš„è¯çš„ `W_out` ä¸å—å½±å“ã€‚

---

## ä¸ƒã€ç›´è§‚æ€»ç»“ï¼ˆæŠŠæ•°å­¦å¯¹åº”åˆ°ä»£ç ï¼‰

* è®¡ç®—å¾—åˆ†ï¼š`score_pos = u_pos.dot(h)`ï¼Œ`score_neg = u_neg.dot(h)`
* è®¡ç®—ç³»æ•°ï¼š`grad_pos = sigmoid(score_pos) - 1`ï¼Œ`grad_neg = sigmoid(score_neg)`
* æ›´æ–° `W_out[pos] -= lr * grad_pos * h`
* æ›´æ–° `W_out[neg] -= lr * grad_neg * h`ï¼ˆå¯¹æ¯ä¸ª negï¼‰
* æ›´æ–° `W_in[center] -= lr * (grad_pos * u_pos + sum(grad_neg * u_neg))`

è¿™äº›å…¬å¼å®Œå…¨å¯¹åº”å‰é¢ä¸¥æ ¼çš„å¾®åˆ†æ¨å¯¼ã€‚

---

å¦‚æœä½ å¸Œæœ›ï¼Œæˆ‘å¯ä»¥æ¥ç€å¸®ä½ åšä¸¤ä»¶äº‹ï¼ˆä»»é€‰ï¼‰ï¼š

1. é’ˆå¯¹**çŸ©é˜µåŒ–æ‰¹é‡å®ç°**ç»™å‡ºå¯¹åº”çš„ PyTorch ä»£ç ç‰‡æ®µï¼ˆæŠŠä¸Šé¢çš„æ ‡é‡å…¬å¼è½¬æˆ `bmm` + å¹¿æ’­çš„å®ç°ï¼ŒåŒ…å«ç´¢å¼•ç´¯åŠ æ›´æ–°ç­‰ç»†èŠ‚ï¼‰ï¼›
2. æ¨å¯¼å¹¶ç»™å‡º**æ•°å€¼ç¨³å®šæ€§æ”¹è¿›**ï¼ˆä¾‹å¦‚å¯¹ (\log\sigma(\cdot)) çš„æ•°å€¼ç¨³å®šå†™æ³•ã€é¿å… overflow/underflow çš„æŠ€å·§ï¼‰ã€‚

ä½ æƒ³å…ˆçœ‹å“ªä¸€ä¸ªï¼Ÿ



éå¸¸å¥½çš„è§‚å¯Ÿ ğŸ‘
ä½ æ³¨æ„åˆ°çš„é—®é¢˜å…¶å®éå¸¸å…¸å‹ï¼š**Word2Vec è´Ÿé‡‡æ ·ç‰ˆçš„ loss çœ‹ä¼¼å¤§ï¼Œä½†ä¸ä¸€å®šæ„å‘³ç€æ²¡å­¦åˆ°ä¸œè¥¿ï¼›ä¸è¿‡è¿™é‡Œç¡®å®è¿˜æ²¡æ”¶æ•›å¥½**ã€‚
æˆ‘ä»¬æ¥ä¸€æ­¥æ­¥åˆ†æä¸ºä»€ä¹ˆä¼šå‡ºç° loss â‰ˆ 13ã€ç›¸ä¼¼åº¦ä¸æ˜æ˜¾ï¼Œä»¥åŠå¦‚ä½•æ”¹è¿›ã€‚

---

## ğŸ§© ä¸€ã€ä¸ºä»€ä¹ˆ loss é«˜ï¼Ÿ

åœ¨è´Ÿé‡‡æ · Skip-Gram ä¸­ï¼ŒæŸå¤±å‡½æ•°æ˜¯ï¼š

[
L = -\log \sigma(v_c^\top u_{pos}) - \sum_{i=1}^{k} \log \sigma(-v_c^\top u_{neg_i})
]

å¯¹äºæ¯ä¸ªæ­£æ ·æœ¬ï¼š

* å¦‚æœæ¨¡å‹éšæœºåˆå§‹åŒ–ï¼Œ(v_c^\top u_{pos}) å’Œ (v_c^\top u_{neg}) éƒ½æ¥è¿‘ 0ï¼›
* (\sigma(0)=0.5)ï¼Œäºæ˜¯ï¼š
  [
  L \approx -\log(0.5) - k \log(0.5) = (k+1)\log(2)
  ]
  ä¸¾ä¾‹ï¼šè‹¥è´Ÿé‡‡æ · k=5
  [
  L \approx 6 \times 0.693 â‰ˆ 4.16
  ]

ä½†ä½ è§‚å¯Ÿåˆ° lossâ‰ˆ13ï¼Œè¯´æ˜ï¼š

1. **æ¯ä¸ª batch ä¸­çš„æ­£è´Ÿæ ·æœ¬æ•°é‡è¾ƒå¤šï¼ˆbatch loss æ˜¯å‡å€¼ï¼Œä½†ç§¯ç´¯è¯¯å·®å¤§ï¼‰**ï¼›
2. **å­¦ä¹ ç‡è¾ƒé«˜æˆ–ä¸ç¨³å®šï¼Œæƒé‡åˆ†å¸ƒåç§»**ï¼›
3. **corpus å¤ªå°**ï¼Œæ¨¡å‹æ ¹æœ¬æ²¡æœºä¼šå­¦åˆ°ç¨³å®šåˆ†å¸ƒã€‚

---

## ğŸ§  äºŒã€å½“å‰ç‰ˆæœ¬å­˜åœ¨çš„ä¸»è¦é—®é¢˜

1. **è¯­æ–™å¤ªå°**

   * ä½ çš„ corpus åªæœ‰å‡ åä¸ªè¯ã€‚
   * Word2Vec éœ€è¦è‡³å°‘ **å‡ åƒåˆ°å‡ ä¸‡æ¬¡è¯å…±ç°** æ‰èƒ½å­¦åˆ°â€œè¯­ä¹‰ç›¸ä¼¼æ€§â€ã€‚
   * å¦åˆ™å‘é‡å‡ ä¹å…¨æ˜¯éšæœºå™ªå£°ã€‚

2. **æœªå½’ä¸€åŒ–é‡‡æ ·æ¦‚ç‡**

   * è´Ÿé‡‡æ ·æ—¶æˆ‘ä»¬ä½¿ç”¨äº†å›ºå®šçš„æ¦‚ç‡è¡¨ï¼ˆf(w)^0.75ï¼‰ï¼Œ
     ä½† corpus å°æ—¶ï¼ŒæŸäº›è¯å¯èƒ½å æ¯”å¼‚å¸¸é«˜ï¼ˆå¯¼è‡´è´Ÿæ ·æœ¬å¤±è¡¡ï¼‰ã€‚

3. **å­¦ä¹ ç‡å¤ªå¤§**

   * `lr=0.01` å¯¹äºéšæœºåˆå§‹åŒ–çš„ä¸¤ä¸ªçŸ©é˜µ (W_in/W_out) æ¥è¯´å¤ªæ¿€è¿›ï¼›
   * ç”±äºæ¢¯åº¦æ›´æ–°äº’ç›¸æ”¾å¤§ï¼Œå®¹æ˜“éœ‡è¡ï¼Œloss ä¸é™ã€‚

4. **æ²¡æœ‰æ¢¯åº¦è£å‰ª**

   * è®­ç»ƒåˆæœŸæŸäº› dot product å€¼åå¤§æ—¶ï¼Œlog-sigmoid ä¼šæ¥è¿‘ -âˆã€‚

---

## âš™ï¸ ä¸‰ã€æ”¹è¿›å»ºè®®

ä¸‹é¢è¿™äº›æ”¹æ³•èƒ½æ˜¾è‘—æ”¹å–„æ•ˆæœï¼š

### âœ… 1. ä½¿ç”¨æ›´å¤§çš„è¯­æ–™

ä¾‹å¦‚ï¼š

```python
from nltk.corpus import brown
corpus = [" ".join(sent) for sent in brown.sents()[:10000]]
```

ç”¨è‡ªç„¶è¯­è¨€çš„ä¸Šä¸‹æ–‡æ‰èƒ½å­¦åˆ°è¯­ä¹‰å…³ç³»ã€‚

---

### âœ… 2. é™ä½å­¦ä¹ ç‡

```python
optimizer = optim.Adam(params, lr=0.002)
```

---

### âœ… 3. æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢çˆ†ç‚¸ï¼‰

```python
torch.nn.utils.clip_grad_norm_(params, 5.0)
```

---

### âœ… 4. åŠ å…¥æ‰¹æ¬¡å½’ä¸€åŒ– (BatchNorm-like trick)

åœ¨æ›´æ–°å‰å…ˆå¯¹æ¯ä¸ªå‘é‡è¿›è¡Œç®€å•å½’ä¸€åŒ–ï¼š

```python
with torch.no_grad():
    W_in.data = W_in.data / (torch.norm(W_in.data, dim=1, keepdim=True) + 1e-9)
    W_out.data = W_out.data / (torch.norm(W_out.data, dim=1, keepdim=True) + 1e-9)
```

---

### âœ… 5. éªŒè¯è®­ç»ƒè¶‹åŠ¿

ç”¨ä¸‹é¢çš„ä»£ç å¿«é€ŸæŸ¥çœ‹ loss è¶‹åŠ¿ï¼š

```python
import matplotlib.pyplot as plt

losses = []  # åœ¨æ¯ä¸ª epoch append(loss)
plt.plot(losses)
plt.xlabel("epoch")
plt.ylabel("loss")
```

---

## ğŸ” å››ã€å¦‚ä½•éªŒè¯æ¨¡å‹æ˜¯å¦åœ¨â€œå­¦â€

ä¸è¦åªçœ‹ lossï¼Œè¦çœ‹ **ç›¸ä¼¼è¯ç©ºé—´å˜åŒ–è¶‹åŠ¿**ï¼š

ä¾‹å¦‚ï¼š

```python
for epoch in range(num_epochs):
    ...
    if epoch % 10 == 0:
        print("similar to 'do':", most_similar("do"))
        print("similar to 'you':", most_similar("you"))
```

éšç€è®­ç»ƒï¼š

* åˆæœŸï¼šç»“æœéšæœºï¼›
* ä¸­æœŸï¼šè¯­ä¹‰é è¿‘ï¼›
* åæœŸï¼šç¨³å®šç›¸ä¼¼ç»„ï¼ˆ`do`ã€`can`ã€`work`ã€`love` ç­‰ï¼‰ã€‚

---

## ğŸ§ª äº”ã€æƒ³è¦çœŸæ­£æ”¶æ•›æ•ˆæœå¥½çš„ç‰ˆæœ¬

è¦è¾¾åˆ°è®ºæ–‡çº§è¡¨ç°ï¼Œéœ€åŒæ—¶æ»¡è¶³ï¼š

| ç»„ä»¶   | æ¨èåšæ³•                      |
| ---- | ------------------------- |
| æ•°æ®   | è‡³å°‘ 1e6 è¯                  |
| è´Ÿé‡‡æ ·æ•° | 10-15                     |
| å­¦ä¹ ç‡  | 1e-3~1e-4                 |
| ä¼˜åŒ–å™¨  | AdamW æˆ– SGD(momentum=0.9) |
| æ­£åˆ™åŒ–  | L2 æˆ–å‘é‡å½’ä¸€åŒ–                 |
| æŸå¤±å¹³æ»‘ | log-sigmoid ç¨³å®šæ€§å¢å¼º         |

---

## âœ… å…­ã€æ€»ç»“

| é—®é¢˜        | åŸå›              | å¯¹ç­–              |
| --------- | -------------- | --------------- |
| loss â‰ˆ 13 | è¯­æ–™å°ã€å­¦ä¹ ç‡å¤§ã€è´Ÿé‡‡æ ·å¤±è¡¡ | æ¢å¤§è¯­æ–™ã€é™ lrã€å½’ä¸€åŒ–é‡‡æ · |
| ç›¸ä¼¼åº¦ä¹±      | å‘é‡ä»æœªå­¦åˆ°å…±ç°ç»“æ„     | å¤šè®­ç»ƒ + è¯­æ–™ä¸°å¯Œ      |
| æ¨¡å‹éœ‡è¡      | æ¢¯åº¦è¿‡å¤§           | ä½¿ç”¨æ¢¯åº¦è£å‰ª + å½’ä¸€åŒ–    |

---

æˆ‘å¯ä»¥å¸®ä½ åŸºäºè¿™ä»½ä»£ç æ”¹å‡ºä¸€ä¸ª **å¯ç¨³å®šæ”¶æ•›çš„ batch è´Ÿé‡‡æ ·ç‰ˆ Word2Vec**ï¼ˆå®Œæ•´å¯è®­ç»ƒã€å« loss æ›²çº¿ç»˜åˆ¶å’Œç›¸ä¼¼åº¦è·Ÿè¸ªï¼‰ï¼Œè¦æˆ‘å¸®ä½ å†™è¿™ä»½â€œæ”¹è¿›ç‰ˆâ€å—ï¼Ÿ


å¥½çš„ï¼Œä¸‹é¢æˆ‘å¸®ä½ æ”¹é€ æˆ**åŸºäº `Dataset` + `DataLoader`** çš„ç‰ˆæœ¬ï¼Œèƒ½æ›´å¥½æ”¯æŒå¤§è¯­æ–™è®­ç»ƒã€‚ä½ å¯ä»¥å…ˆè·‘è¿™ä¸ªç‰ˆæœ¬ï¼Œç„¶åå¦‚æœéœ€è¦ï¼Œæˆ‘è¿˜èƒ½å¸®ä½ æ›¿æ¢ä¸ºçœŸå®è¯­æ–™ï¼ˆä¾‹å¦‚ text8ï¼‰è¿›è¡Œè®­ç»ƒã€‚

---

## âœ”ï¸ æ”¹ç‰ˆä»£ç ï¼šä½¿ç”¨ `Dataset` + `DataLoader`

```python
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import Counter
import math
from torch.utils.data import Dataset, DataLoader

# é…ç½®
seed = 42
random.seed(seed)
torch.manual_seed(seed)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# åŠ è½½è¯­æ–™ï¼ˆä½ å¯æ›¿æ¢ä¸ºå¤§è¯­æ–™ï¼‰
raw_corpus = [
    "we are what we repeatedly do excellence then is not an act but a habit",
    "the only way to do great work is to love what you do",
    "if you can dream it you can do it",
    "do what you can with what you have where you are",
    "you are never too old to set another goal or to dream a new dream",
    "whether you think you can or you think you can not you are right",
]

repeat_factor = 200
corpus = []
for _ in range(repeat_factor):
    corpus.extend(raw_corpus)

tokens = [w for sent in corpus for w in sent.lower().split()]
vocab = sorted(list(set(tokens)))
vocab_size = len(vocab)
word2idx = {w: i for i, w in enumerate(vocab)}
idx2word = {i: w for w, i in word2idx.items()}
print(f"Vocab size: {vocab_size}, token count: {len(tokens)}")

# ç”Ÿæˆè®­ç»ƒå¯¹
window_size = 2
pairs = []
for i, _ in enumerate(tokens):
    center_word = tokens[i]
    left = max(0, i - window_size)
    right = min(len(tokens), i + window_size + 1)
    for j in range(left, right):
        if j == i:
            continue
        pairs.append((word2idx[center_word], word2idx[tokens[j]]))
print("Total pairs:", len(pairs))

# è´Ÿé‡‡æ ·åˆ†å¸ƒ
freq = Counter(tokens)
freq_list = torch.tensor([freq[idx2word[i]] for i in range(vocab_size)], dtype=torch.float)
unigram = freq_list.pow(0.75)
unigram = unigram / unigram.sum()
unigram = unigram.to(device)

# Dataset å®šä¹‰
class SkipGramDataset(Dataset):
    def __init__(self, pairs):
        self.pairs = pairs
    def __len__(self):
        return len(self.pairs)
    def __getitem__(self, idx):
        return self.pairs[idx]

dataset = SkipGramDataset(pairs)
batch_size = 256
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)

# æ¨¡å‹å‚æ•°ï¼ˆä¸ç”¨ nn.Embeddingï¼‰
embedding_dim = 100
W_in = nn.Parameter(torch.randn(vocab_size, embedding_dim, device=device) * 0.01)
W_out = nn.Parameter(torch.randn(vocab_size, embedding_dim, device=device) * 0.01)

neg_samples = 10
lr = 0.002
weight_decay = 1e-6
grad_clip = 1.0
epochs = 10

optimizer = optim.Adam([W_in, W_out], lr=lr, weight_decay=weight_decay)
sigmoid = torch.sigmoid

# è®­ç»ƒ
loss_history = []
for epoch in range(1, epochs+1):
    total_loss = 0.0
    for centers, contexts in dataloader:
        centers = centers.to(device)
        contexts = contexts.to(device)
        B = centers.size(0)

        v_c = W_in[centers]     # (B, D)
        v_pos = W_out[contexts] # (B, D)
        neg_idx = torch.multinomial(unigram, B * neg_samples, replacement=True).view(B, neg_samples).to(device)
        v_neg = W_out[neg_idx]  # (B, K, D)

        # æ­£æ ·æœ¬å¾—åˆ†
        score_pos = torch.sum(v_c * v_pos, dim=1)            # (B,)
        loss_pos = -torch.log(sigmoid(score_pos) + 1e-10)

        # è´Ÿæ ·æœ¬å¾—åˆ†
        score_neg = torch.bmm(v_neg, v_c.unsqueeze(2)).squeeze(2)  # (B, K)
        loss_neg = -torch.sum(torch.log(sigmoid(-score_neg) + 1e-10), dim=1)

        loss = torch.mean(loss_pos + loss_neg)

        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_([W_in, W_out], max_norm=grad_clip)
        optimizer.step()

        total_loss += loss.item() * B

    avg_loss = total_loss / len(dataset)
    loss_history.append(avg_loss)

    # å‘é‡å½’ä¸€åŒ–
    with torch.no_grad():
        W_in.data = W_in.data / (torch.norm(W_in.data, dim=1, keepdim=True) + 1e-9)
        W_out.data = W_out.data / (torch.norm(W_out.data, dim=1, keepdim=True) + 1e-9)

    print(f"Epoch {epoch}/{epochs} avg_loss={avg_loss:.4f}")
    # æ‰“å°å‡ ä¸ª probe è¯çš„ç›¸ä¼¼åº¦
    for probe in ["do", "you", "dream", "work", "love"]:
        if probe in word2idx:
            sims = []
            idx = word2idx[probe]
            vec = W_in[idx]
            cos = torch.matmul(W_in, vec) / (torch.norm(W_in, dim=1) * torch.norm(vec) + 1e-9)
            topk_idx = torch.topk(cos, 5+1).indices.tolist()
            res = [(idx2word[i], float(cos[i].cpu())) for i in topk_idx if i != idx][:5]
            print("  similar to", probe, ":", res)
    print("-" * 50)

# ç»˜åˆ¶ loss æ›²çº¿
import matplotlib.pyplot as plt
plt.plot(loss_history)
plt.xlabel("epoch")
plt.ylabel("avg loss per token")
plt.title("Training loss")
plt.show()

# æŸ¥çœ‹æœ€ç»ˆç›¸ä¼¼è¯
print("Most similar to 'do':", [(w, float(torch.matmul(W_in[word2idx[w]], W_in[word2idx["do"]]) /
                                              (torch.norm(W_in[word2idx[w]])*torch.norm(W_in[word2idx["do"]])+1e-9))) 
                                  for w in ["you", "can", "work", "dream", "love"]])
```

---

ä½ å…ˆè¿è¡Œä¸Šé¢è¿™ä»½ä»£ç ã€‚
è¿è¡Œåè¯·æŸ¥çœ‹ä»¥ä¸‹æŒ‡æ ‡ï¼š

* æ¯ä¸ª epoch è¾“å‡ºçš„ `avg_loss` æ˜¯å¦åœ¨ä¸‹é™ï¼Ÿ
* `probe` è¯ï¼ˆå¦‚ â€œdoâ€, â€œyouâ€ç­‰ï¼‰æ˜¯å¦å¼€å§‹æœ‰è¯­ä¹‰ç›¸ä¼¼çš„è¯ï¼ˆä¾‹å¦‚ â€œdoâ€ å¯èƒ½è¿‘ä¼¼ â€œcanâ€/â€œmakeâ€ï¼‰ï¼Ÿ
* loss æ›²çº¿æ˜¯å¦æ”¶æ•›ï¼ˆè™½ç„¶è¯­æ–™å°ï¼Œä½†åº”è¯¥æœ‰ä¸‹é™è¶‹åŠ¿ï¼‰ã€‚

å¦‚æœè¿è¡Œåä½ å‘ç° **loss æ²¡æœ‰ä¸‹é™** æˆ– **ç›¸ä¼¼è¯ä»éšæœº**ï¼Œæˆ‘ä»¬å¯ä»¥å†è¿›ä¸€æ­¥è°ƒæ•´ï¼š

* å¢å¤§ `repeat_factor`ï¼ˆä½¿æ•°æ®æ›´å¤šï¼‰
* å¢åŠ  `neg_samples`
* é™ä½ `lr`
* å¢å¤§ `embedding_dim`
* ä½¿ç”¨çœŸå®çš„å¤§è¯­æ–™ï¼ˆä¾‹å¦‚ text8ï¼‰

ä½ å…ˆè·‘èµ·æ¥ç„¶åå‘Šè¯‰æˆ‘ â€œloss æ›²çº¿â€ ä¸ â€œç›¸ä¼¼è¯è¾“å‡ºâ€ çš„æƒ…å†µï¼Œæˆ‘ä»¬å†ä¸€èµ·è°ƒä¼˜ã€‚


æ˜ç™½äº†ã€‚ä¸‹é¢æ˜¯ä¸€ä»½ **å®Œæ•´çš„ PyTorch ç‰ˆ Skipâ€‘Gram + è´Ÿé‡‡æ ·**ï¼Œ**ä½¿ç”¨ text8 è¯­æ–™** çš„ä»£ç ã€‚ä½ å¯ä»¥ç›´æ¥è¿è¡Œï¼ˆå¦‚æœæœºå™¨èµ„æºå…è®¸ï¼‰è¿›è¡Œè®­ç»ƒã€‚

> æ³¨æ„ï¼šå› ä¸º text8 è¯­æ–™é‡è¾ƒå¤§ï¼Œè®­ç»ƒå¯èƒ½éœ€è¦ **è¾ƒé•¿æ—¶é—´** æˆ– **é™ä½ batch_size / epoch** æ¥å¿«é€Ÿæµ‹è¯•ã€‚

---

## âœ… å®Œæ•´ä»£ç 

```python
import torch
import torch.nn as nn
import torch.optim as optim
import random
import math
from collections import Counter
from torch.utils.data import Dataset, DataLoader
import gensim.downloader as api

# ========== é…ç½® ==========
seed = 42
random.seed(seed)
torch.manual_seed(seed)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# ========== 1) åŠ è½½ text8 è¯­æ–™ ==========
dataset = api.load("text8")           # è¿”å›ä¸€ä¸ª iterableï¼Œæ¯ä¸ª element æ˜¯ list of str :contentReference[oaicite:1]{index=1}
# å°†å…¶è½¬ä¸º tokens åˆ—è¡¨
tokens = []
for sent in dataset:
    tokens.extend(sent)
print("Total tokens in text8:", len(tokens))

# ========== 2) æ„å»ºè¯è¡¨ ==========
vocab = sorted(list(set(tokens)))
vocab_size = len(vocab)
word2idx = {w:i for i, w in enumerate(vocab)}
idx2word = {i:w for w,i in word2idx.items()}
print("Vocab size:", vocab_size)

# ========== 3) ç”Ÿæˆè®­ç»ƒå¯¹ (skipâ€‘gram) ==========
window_size = 2
pairs = []
for i, w in enumerate(tokens):
    center_idx = word2idx[w]
    left = max(0, i - window_size)
    right = min(len(tokens), i + window_size + 1)
    for j in range(left, right):
        if j == i:
            continue
        context_idx = word2idx[tokens[j]]
        pairs.append((center_idx, context_idx))
print("Total training pairs:", len(pairs))

# ========== 4) è´Ÿé‡‡æ ·åˆ†å¸ƒ ==========
freq = Counter(tokens)
freq_list = torch.tensor([freq[idx2word[i]] for i in range(vocab_size)], dtype=torch.float)
unigram = freq_list.pow(0.75)
unigram = unigram / unigram.sum()
unigram = unigram.to(device)

# ========== 5) Dataset + DataLoader ==========
class SkipGramDataset(Dataset):
    def __init__(self, pairs):
        self.pairs = pairs
    def __len__(self):
        return len(self.pairs)
    def __getitem__(self, idx):
        return self.pairs[idx]

batch_size = 512    # ä½ å¯æ ¹æ®æ˜¾å­˜è°ƒæ•´
dataset_obj = SkipGramDataset(pairs)
dataloader = DataLoader(dataset_obj, batch_size=batch_size, shuffle=True, drop_last=True)

# ========== 6) æ¨¡å‹å‚æ•°ï¼ˆä¸ç”¨ nn.Embeddingï¼‰ ==========
embedding_dim = 128
W_in = nn.Parameter(torch.randn(vocab_size, embedding_dim, device=device) * 0.01)
W_out = nn.Parameter(torch.randn(vocab_size, embedding_dim, device=device) * 0.01)

neg_samples = 10
lr = 0.001
weight_decay = 1e-6
grad_clip = 5.0
epochs = 5    # åˆæ¬¡æµ‹è¯•å¯å°‘è·‘å‡ è½®

optimizer = optim.Adam([W_in, W_out], lr=lr, weight_decay=weight_decay)
sigmoid = torch.sigmoid

# ========== 7) è®­ç»ƒå¾ªç¯ ==========
loss_history = []
for epoch in range(1, epochs+1):
    total_loss = 0.0
    for centers, contexts in dataloader:
        centers = centers.to(device, dtype=torch.long)
        contexts = contexts.to(device, dtype=torch.long)
        B = centers.size(0)

        v_c = W_in[centers]        # (B, D)
        v_pos = W_out[contexts]    # (B, D)

        neg_idx = torch.multinomial(unigram, B * neg_samples, replacement=True).view(B, neg_samples).to(device)
        v_neg = W_out[neg_idx]     # (B, K, D)

        # æ­£æ ·æœ¬æŸå¤±
        score_pos = torch.sum(v_c * v_pos, dim=1)                 # (B,)
        loss_pos = -torch.log(sigmoid(score_pos) + 1e-10)         # (B,)

        # è´Ÿæ ·æœ¬æŸå¤±
        score_neg = torch.bmm(v_neg, v_c.unsqueeze(2)).squeeze(2)   # (B, K)
        loss_neg = -torch.sum(torch.log(sigmoid(-score_neg) + 1e-10), dim=1)  # (B,)

        # å¹³å‡ batch loss
        loss = torch.mean(loss_pos + loss_neg)

        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_([W_in, W_out], max_norm=grad_clip)
        optimizer.step()

        total_loss += loss.item() * B

    avg_loss = total_loss / len(dataset_obj)
    loss_history.append(avg_loss)

    # å‘é‡å½’ä¸€åŒ–
    with torch.no_grad():
        W_in.data = W_in.data / (torch.norm(W_in.data, dim=1, keepdim=True) + 1e-9)
        W_out.data = W_out.data / (torch.norm(W_out.data, dim=1, keepdim=True) + 1e-9)

    print(f"Epoch {epoch}/{epochs}  avg_loss = {avg_loss:.4f}")

# ========== 8) ç»˜åˆ¶ loss ==========
import matplotlib.pyplot as plt
plt.plot(loss_history)
plt.xlabel("Epoch")
plt.ylabel("Avg loss per sample")
plt.title("Word2Vec Negative Sampling Training Loss")
plt.show()

# ========== 9) æŸ¥çœ‹ç›¸ä¼¼è¯ ==========
def most_similar(word, topk=10):
    if word not in word2idx:
        return []
    idx = word2idx[word]
    vec = W_in[idx]
    sims = torch.matmul(W_in, vec) / (torch.norm(W_in, dim=1) * torch.norm(vec) + 1e-9)
    topk_idx = torch.topk(sims, topk+1).indices.tolist()
    res = []
    for i in topk_idx:
        if i != idx:
            res.append((idx2word[i], float(sims[i].cpu())))
        if len(res) >= topk:
            break
    return res

for probe in ["king", "queen", "woman", "man", "computer", "data"]:
    print("Most similar to", probe, ":", most_similar(probe, topk=5))
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

* è®­ç»ƒç”¨æ—¶ï¼štext8 çº¦ 17â€¯M è¯ï¼›ä»¥ batch_size=512ã€epochs=5 è®­ç»ƒå¯èƒ½ä¹Ÿéœ€è¦æ•°åˆ†é’Ÿæˆ–æ›´å¤šï¼Œè§† GPU/CPU è€Œå®šã€‚
* è‹¥è®­ç»ƒæ—¶é—´å¤ªé•¿ï¼Œå¯å…ˆ **å‡å°‘ `vocab_size`**ï¼ˆåªä¿ç•™é«˜é¢‘è¯ï¼Œå¦‚å‡ºç°æ¬¡æ•° > 50ï¼‰ï¼Œæˆ–è€… **å‡å° embedding_dim**ã€**å‡å° batch_size**ã€‚
* ä¸Šåˆåˆæ¬¡è¿è¡Œæ—¶ä½ å¯èƒ½çœ‹åˆ° `avg_loss` å¾ˆé«˜ï¼Œä½†åªè¦å‘ˆä¸‹é™è¶‹åŠ¿ï¼Œå¹¶ä¸”ç›¸ä¼¼è¯æœ‰æ„ä¹‰å³å¯ã€‚
* è‹¥ç›¸ä¼¼è¯çœ‹èµ·æ¥ä»ä¹±ï¼Œå»ºè®®å†å¢åŠ  epoch æ•°ã€å¢å¤§è¯­æ–™æˆ–å¢åŠ è´Ÿé‡‡æ ·æ•°ã€‚

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½  **æä¾›ä¸€ä¸ªç”¨äºæ–‡æœ¬é¢„å¤„ç†ã€åªä¿ç•™é«˜é¢‘è¯ & é™åˆ¶è¯è¡¨å¤§å°ï¼ˆå¦‚å‰1ä¸‡è¯ï¼‰** çš„ç‰ˆæœ¬ï¼Œè¿™æ ·è®­ç»ƒæ›´å¿«ä¹Ÿæ›´å®¹æ˜“çœ‹åˆ°æ•ˆæœã€‚ä½ è¦è¿™ä¸ªå—ï¼Ÿ




å¥½çš„ï¼Œæˆ‘å¸®ä½ æŠŠä¹‹å‰çš„ **text8 è´Ÿé‡‡æ · Word2Vec** å®Œæ•´ä»£ç æ”¹å†™æˆ **æ”¯æŒåªä¿ç•™å‰ 10000 é«˜é¢‘è¯ + TensorBoard å¯è§†åŒ–** çš„ç‰ˆæœ¬ï¼Œå¹¶ä¸”ä¸ç”¨ `nn.Embedding`ï¼Œå¯ä»¥ç›´æ¥è®­ç»ƒã€‚

---

## å®Œæ•´ä»£ç 

```python
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import Counter
from torch.utils.data import Dataset, DataLoader
import gensim.downloader as api
import matplotlib.pyplot as plt
from torch.utils.tensorboard import SummaryWriter

# ================= é…ç½® =================
seed = 42
random.seed(seed)
torch.manual_seed(seed)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# ================= 1) åŠ è½½ text8 è¯­æ–™ =================
dataset = api.load("text8")
tokens = []
for sent in dataset:
    tokens.extend(sent)
print("Total tokens in text8:", len(tokens))

# ================= 2) åªä¿ç•™å‰ 10000 é«˜é¢‘è¯ =================
freq = Counter(tokens)
most_common = freq.most_common(10000)
vocab = [w for w, _ in most_common]
vocab_size = len(vocab)
word2idx = {w:i for i, w in enumerate(vocab)}
idx2word = {i:w for w,i in word2idx.items()}

# è¿‡æ»¤ tokensï¼Œåªä¿ç•™é«˜é¢‘è¯
tokens = [w for w in tokens if w in word2idx]
print(f"Filtered token count: {len(tokens)}, vocab size: {vocab_size}")

# ================= 3) ç”Ÿæˆè®­ç»ƒå¯¹ =================
window_size = 2
pairs = []
for i, w in enumerate(tokens):
    center_idx = word2idx[w]
    left = max(0, i - window_size)
    right = min(len(tokens), i + window_size + 1)
    for j in range(left, right):
        if j == i:
            continue
        context_word = tokens[j]
        if context_word not in word2idx:
            continue
        context_idx = word2idx[context_word]
        pairs.append((center_idx, context_idx))
print("Total training pairs after filtering:", len(pairs))

# ================= 4) è´Ÿé‡‡æ ·åˆ†å¸ƒ =================
freq_list = torch.tensor([freq[w] for w in vocab], dtype=torch.float)
unigram = freq_list.pow(0.75)
unigram = unigram / unigram.sum()
unigram = unigram.to(device)

# ================= 5) Dataset + DataLoader =================
class SkipGramDataset(Dataset):
    def __init__(self, pairs):
        self.pairs = pairs
    def __len__(self):
        return len(self.pairs)
    def __getitem__(self, idx):
        return self.pairs[idx]

batch_size = 512
dataset_obj = SkipGramDataset(pairs)
dataloader = DataLoader(dataset_obj, batch_size=batch_size, shuffle=True, drop_last=True)

# ================= 6) æ¨¡å‹å‚æ•°ï¼ˆä¸ç”¨ nn.Embeddingï¼‰ =================
embedding_dim = 128
W_in = nn.Parameter(torch.randn(vocab_size, embedding_dim, device=device) * 0.01)
W_out = nn.Parameter(torch.randn(vocab_size, embedding_dim, device=device) * 0.01)

neg_samples = 10
lr = 0.001
weight_decay = 1e-6
grad_clip = 5.0
epochs = 5

optimizer = optim.Adam([W_in, W_out], lr=lr, weight_decay=weight_decay)
sigmoid = torch.sigmoid

# ================= 7) TensorBoard =================
writer = SummaryWriter(log_dir="./runs/word2vec_ns")

# ================= 8) è®­ç»ƒå¾ªç¯ =================
loss_history = []
for epoch in range(1, epochs+1):
    total_loss = 0.0
    for step, (centers, contexts) in enumerate(dataloader):
        centers = centers.to(device)
        contexts = contexts.to(device)
        B = centers.size(0)

        v_c = W_in[centers]        # (B, D)
        v_pos = W_out[contexts]    # (B, D)

        neg_idx = torch.multinomial(unigram, B * neg_samples, replacement=True).view(B, neg_samples).to(device)
        v_neg = W_out[neg_idx]     # (B, K, D)

        # æ­£æ ·æœ¬æŸå¤±
        score_pos = torch.sum(v_c * v_pos, dim=1)
        loss_pos = -torch.log(sigmoid(score_pos) + 1e-10)

        # è´Ÿæ ·æœ¬æŸå¤±
        score_neg = torch.bmm(v_neg, v_c.unsqueeze(2)).squeeze(2)
        loss_neg = -torch.sum(torch.log(sigmoid(-score_neg) + 1e-10), dim=1)

        loss = torch.mean(loss_pos + loss_neg)

        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_([W_in, W_out], max_norm=grad_clip)
        optimizer.step()

        total_loss += loss.item() * B

        # æ¯ batch å†™å…¥ TensorBoard
        global_step = (epoch-1)*len(dataloader) + step
        writer.add_scalar("Loss/Batch", loss.item(), global_step)

    avg_loss = total_loss / len(dataset_obj)
    loss_history.append(avg_loss)
    writer.add_scalar("Loss/Epoch", avg_loss, epoch)
    print(f"Epoch {epoch}/{epochs}  avg_loss = {avg_loss:.4f}")

    # å‘é‡å½’ä¸€åŒ–
    with torch.no_grad():
        W_in.data = W_in.data / (torch.norm(W_in.data, dim=1, keepdim=True) + 1e-9)
        W_out.data = W_out.data / (torch.norm(W_out.data, dim=1, keepdim=True) + 1e-9)

# ================= 9) ç»˜åˆ¶ loss =================
plt.plot(loss_history)
plt.xlabel("Epoch")
plt.ylabel("Avg loss per sample")
plt.title("Word2Vec Negative Sampling Training Loss")
plt.show()

# ================= 10) æŸ¥çœ‹ç›¸ä¼¼è¯ =================
def most_similar(word, topk=10):
    if word not in word2idx:
        return []
    idx = word2idx[word]
    vec = W_in[idx]
    sims = torch.matmul(W_in, vec) / (torch.norm(W_in, dim=1) * torch.norm(vec) + 1e-9)
    topk_idx = torch.topk(sims, topk+1).indices.tolist()
    res = []
    for i in topk_idx:
        if i != idx:
            res.append((idx2word[i], float(sims[i].cpu())))
        if len(res) >= topk:
            break
    return res

probe_words = ["king", "queen", "woman", "man", "computer", "data"]
for probe in probe_words:
    print("Most similar to", probe, ":", most_similar(probe, topk=5))

# ================= 11) å…³é—­ TensorBoard =================
writer.close()
```

---

### âœ… ä½¿ç”¨è¯´æ˜

1. è®­ç»ƒæ—¶ loss ä¼šè¾“å‡ºåˆ°æ§åˆ¶å°ï¼Œå¹¶è®°å½•åœ¨ TensorBoardã€‚
2. ç›¸ä¼¼è¯ä¹Ÿä¼šéšè®­ç»ƒå¾—åˆ°ï¼Œä½ å¯ä»¥æŸ¥çœ‹è®­ç»ƒæ•ˆæœã€‚
3. å¯åŠ¨ TensorBoardï¼š

```bash
tensorboard --logdir=runs
```

ç„¶åè®¿é—®æµè§ˆå™¨ `http://localhost:6006` æŸ¥çœ‹ï¼š

* Loss æ›²çº¿ï¼ˆBatch/ Epochï¼‰
* è®­ç»ƒè¿›ç¨‹ä¸­çš„è¯å‘é‡ç›¸ä¼¼åº¦

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ å†**åŠ ä¸€ä¸ª probe è¯çš„ç›¸ä¼¼åº¦éš epoch å˜åŒ–çš„æ›²çº¿**ï¼Œæ–¹ä¾¿åœ¨ TensorBoard ä¸Šç›´è§‚è§‚å¯Ÿè¯å‘é‡æ•ˆæœã€‚

ä½ å¸Œæœ›æˆ‘åŠ å—ï¼Ÿ


éå¸¸æ£’çš„é—®é¢˜ ğŸ‘
è¿™æ­£æ˜¯ **Word2Vec Skip-Gram** çš„æ•°å­¦æ ¸å¿ƒï¼š
ä¸ºä»€ä¹ˆå‰å‘ä¼ æ’­è¦åš `u = W_out.T @ h` â€”â€” ä¹Ÿå°±æ˜¯è¾“å…¥å‘é‡ï¼ˆä¸­å¿ƒè¯ï¼‰å»ç‚¹ä¹˜è¾“å‡ºçŸ©é˜µã€‚

æˆ‘ä»¬ä¸€æ­¥æ­¥æ‹†å¼€è®²ã€‚

---

## ğŸ§  1ï¸âƒ£ æ¨¡å‹ç›®æ ‡å›é¡¾

Skip-Gram çš„ç›®æ ‡æ˜¯å­¦ä¸€ä¸ªæ¡ä»¶æ¦‚ç‡ï¼š

[
P(w_o | w_c)
]

ä¹Ÿå°±æ˜¯ï¼š**ç»™å®šä¸­å¿ƒè¯ ( w_c )ï¼Œé¢„æµ‹ä¸Šä¸‹æ–‡è¯ ( w_o )**ã€‚

ç¥ç»ç½‘ç»œæœ€åŸºæœ¬çš„ç»“æ„æ˜¯ï¼š

```
è¾“å…¥ï¼šä¸­å¿ƒè¯ one-hot
â†“
Embedding (W_in)
â†“
éšè—å±‚å‘é‡ h
â†“
è¾“å‡ºå±‚ (W_out)
â†“
softmax â†’ å¾—åˆ°å¯¹æ‰€æœ‰è¯çš„æ¦‚ç‡åˆ†å¸ƒ
```

---

## ğŸ§© 2ï¸âƒ£ çŸ©é˜µç¬¦å·å®šä¹‰

| ç¬¦å·          | å«ä¹‰              | ç»´åº¦     |
| ----------- | --------------- | ------ |
| ( V )       | è¯è¡¨å¤§å°            |        |
| ( D )       | å‘é‡ç»´åº¦            |        |
| ( W_{in} )  | è¾“å…¥æƒé‡çŸ©é˜µ          | (V, D) |
| ( W_{out} ) | è¾“å‡ºæƒé‡çŸ©é˜µ          | (V, D) |
| ( x )       | ä¸­å¿ƒè¯çš„ one-hot å‘é‡ | (V, 1) |

---

## âš™ï¸ 3ï¸âƒ£ å‰å‘ä¼ æ’­æ¨å¯¼

### ï¼ˆ1ï¼‰å–ä¸­å¿ƒè¯çš„å‘é‡

[
h = W_{in}^T x
]
ç”±äº (x) æ˜¯ one-hot å‘é‡ï¼Œåªä¼šé€‰ä¸­ä¸€è¡Œï¼š
[
h = W_{in}[center_idx] \quad (1 \times D)
]

---

### ï¼ˆ2ï¼‰è®¡ç®—æ¯ä¸ªè¯ä½œä¸ºä¸Šä¸‹æ–‡çš„å¾—åˆ†

æˆ‘ä»¬å¸Œæœ›è®¡ç®—ï¼š
æ¯ä¸ªå€™é€‰è¯ (w_o) ä¸å½“å‰ä¸­å¿ƒè¯ (w_c) çš„åŒ¹é…ç¨‹åº¦ã€‚
è¿™ç§â€œåŒ¹é…ç¨‹åº¦â€æœ€è‡ªç„¶çš„æ–¹å¼å°±æ˜¯ **å†…ç§¯**ï¼š

[
u_j = h \cdot W_{out}[j]
]

ä¹Ÿå°±æ˜¯ä¸­å¿ƒè¯å‘é‡ä¸è¾“å‡ºå±‚æ¯ä¸ªè¯çš„å‘é‡çš„ç›¸ä¼¼åº¦ã€‚

å†™æˆçŸ©é˜µå½¢å¼å°±æ˜¯ï¼š

[
u = W_{out} h^T \quad \Rightarrow \quad u = W_{out}^T h
]

> è¿™æ · `u` å°±æ˜¯ä¸€ä¸ªé•¿åº¦ä¸º `vocab_size` çš„å‘é‡ï¼Œæ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€ä¸ªè¯çš„â€œå¾—åˆ†â€ã€‚

---

### ï¼ˆ3ï¼‰softmax å¾—åˆ°æ¦‚ç‡åˆ†å¸ƒ

[
y_{pred} = \text{softmax}(u)
]
è¿™æ ·ï¼š
[
y_{pred}[j] = \frac{e^{u_j}}{\sum_k e^{u_k}}
]
ä»£è¡¨è¯è¡¨ä¸­ç¬¬ j ä¸ªè¯ä½œä¸ºä¸Šä¸‹æ–‡å‡ºç°çš„æ¦‚ç‡ã€‚

---

## ğŸ§­ 4ï¸âƒ£ ç›´è§‚ç†è§£

å¯ä»¥è¿™æ ·ç†è§£è¿™ä¸ª `dot` æ“ä½œï¼š

| éƒ¨åˆ†                | ç±»æ¯”             |
| ----------------- | -------------- |
| `h`               | â€œä¸­å¿ƒè¯çš„è¯­ä¹‰å‘é‡â€     |
| `W_out`           | â€œè¯è¡¨ä¸­æ¯ä¸ªè¯çš„è¯­ä¹‰æ¨¡æ¿â€  |
| `u = W_out.T @ h` | è®¡ç®—ä¸­å¿ƒè¯ä¸æ¯ä¸ªæ¨¡æ¿çš„ç›¸ä¼¼åº¦ |
| `softmax(u)`      | æŠŠè¿™äº›ç›¸ä¼¼åº¦å˜æˆæ¦‚ç‡åˆ†å¸ƒ   |

---

## ğŸ“˜ ä¸¾ä¸ªç®€å•ä¾‹å­

å‡è®¾è¯è¡¨ä¸­æœ‰ 3 ä¸ªè¯ï¼Œç»´åº¦ 2ï¼š

```
W_in  = [[1, 0],
         [0, 1],
         [1, 1]]

W_out = [[1, 2],
         [0, 1],
         [1, 0]]
```

ä¸­å¿ƒè¯æ˜¯ç¬¬ 0 ä¸ªè¯ â†’ `h = [1, 0]`

```
u = W_out.T @ h
  = [[1,0,1],
     [2,1,0]] @ [1,0]
  = [1, 2]
```

è¿™é‡Œ shape ä¸ä¸¥è°¨åªæ˜¯æ¼”ç¤ºå«ä¹‰ï¼š
**æ¯ä¸ªè¾“å‡ºè¯éƒ½ä¸è¾“å…¥ä¸­å¿ƒè¯åšå†…ç§¯ï¼Œå¾—åˆ†é«˜è¯´æ˜æ›´å¯èƒ½æ˜¯ä¸Šä¸‹æ–‡è¯ã€‚**

---

æ€»ç»“ä¸€ä¸‹ ğŸ‘‡

| æ­¥éª¤        | æ•°å­¦å¼                        | å«ä¹‰      |
| --------- | -------------------------- | ------- |
| å–ä¸­å¿ƒè¯å‘é‡    | ( h = W_{in}[center_idx] ) | ä¸­å¿ƒè¯è¯­ä¹‰è¡¨ç¤º |
| è®¡ç®—æ‰€æœ‰è¾“å‡ºè¯å¾—åˆ† | ( u = W_{out}^T h )        | ç›¸ä¼¼åº¦æ‰“åˆ†   |
| softmax   | ( y_{pred} = softmax(u) )  | æ¦‚ç‡åˆ†å¸ƒ    |

---

å¦‚æœä½ æƒ³ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»å‡ºè¿™ä¸ªæµç¨‹çš„â€œå‘é‡ç©ºé—´å›¾â€ï¼Œæ˜¾ç¤ºè¾“å…¥è¯å‘é‡ç‚¹ä¹˜è¾“å‡ºè¯çŸ©é˜µæ˜¯å¦‚ä½•å¾—åˆ°æ¦‚ç‡çš„ã€‚æ˜¯å¦å¸®ä½ ç”»ï¼Ÿ


ğŸ’¯ éå¸¸æ£’çš„é—®é¢˜ï¼Œè¿™ä¸ªç‚¹å¾ˆå¤šäººç¬¬ä¸€æ¬¡å­¦ Word2Vec æ—¶éƒ½ä¼šå›°æƒ‘ï¼š

> â€œW_out æ—¢å« â€˜ä¸Šä¸‹æ–‡è¯çš„å‘é‡â€™ï¼Œåˆå« â€˜æƒé‡çŸ©é˜µâ€™ï¼Œè¿™ä¸å†²çªå—ï¼Ÿâ€

å…¶å®ä¸å†²çªâ€”â€”**å®ƒåŒæ—¶æ‰¿æ‹…äº†â€œå‚æ•°çŸ©é˜µï¼ˆæƒé‡ï¼‰â€å’Œâ€œä¸Šä¸‹æ–‡è¯å‘é‡è¡¨â€çš„åŒé‡è§’è‰²**ã€‚
æˆ‘ä»¬ä¸€ç‚¹ç‚¹æ¥å‰–æ ğŸ‘‡

---

## ğŸ§  ä¸€ã€å…ˆçœ‹ Word2Vec çš„ç¥ç»ç½‘ç»œç»“æ„

åœ¨ Skip-Gram æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬æœ‰ï¼š

```
è¾“å…¥å±‚ï¼ˆone-hotï¼‰
â†“
éšè—å±‚ï¼šW_in
â†“
è¾“å‡ºå±‚ï¼šW_out
â†“
softmax / sigmoid
```

| åç§°      | çŸ©é˜µå½¢çŠ¶   | å«ä¹‰                    |
| ------- | ------ | --------------------- |
| `W_in`  | (V, D) | è¯ä½œä¸ºâ€œä¸­å¿ƒè¯â€æ—¶çš„åµŒå…¥ï¼ˆè¾“å…¥è¯å‘é‡è¡¨ï¼‰  |
| `W_out` | (V, D) | è¯ä½œä¸ºâ€œä¸Šä¸‹æ–‡è¯â€æ—¶çš„åµŒå…¥ï¼ˆè¾“å‡ºè¯å‘é‡è¡¨ï¼‰ |

---

## âš™ï¸ äºŒã€ä¸ºä»€ä¹ˆè¯´ `W_out` æ˜¯æƒé‡çŸ©é˜µ

åœ¨ç¥ç»ç½‘ç»œçš„è§’åº¦ï¼Œ`W_out` æ˜¯ **ä»éšè—å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡çŸ©é˜µ**ã€‚
å³ï¼š
[
u = W_{out}^T h
]
è¿™é‡Œå®ƒçš„æ•°å­¦åŠŸèƒ½æ˜¯ï¼š

> æŠŠéšè—å±‚å‘é‡ h æ˜ å°„æˆä¸€ä¸ªé•¿åº¦ä¸º vocab_size çš„å¾—åˆ†å‘é‡ï¼ˆlogitsï¼‰ã€‚

æ‰€ä»¥ï¼š
âœ… ä»ç½‘ç»œå®ç°è§’åº¦ï¼Œå®ƒæ˜¯**æƒé‡çŸ©é˜µ**ã€‚

---

## ğŸ§© ä¸‰ã€ä¸ºä»€ä¹ˆåˆè¯´å®ƒæ˜¯â€œä¸Šä¸‹æ–‡è¯çš„å‘é‡è¡¨â€

`W_out` çš„æ¯ä¸€è¡Œï¼Œå¯¹åº”è¯è¡¨ä¸­ä¸€ä¸ªè¯ï¼ˆä¾‹å¦‚ â€œappleâ€ã€â€œdataâ€ã€â€œrunâ€ï¼‰ã€‚
å½“è¿™ä¸ªè¯ä½œä¸º**ä¸Šä¸‹æ–‡è¯ï¼ˆç›®æ ‡è¯ï¼‰**å‡ºç°æ—¶ï¼Œå®ƒçš„è¡Œå‘é‡ `W_out[word_idx]`
å°±æ˜¯å®ƒåœ¨â€œè¾“å‡ºç©ºé—´â€ä¸­çš„è¯­ä¹‰å‘é‡è¡¨ç¤º ( u_{word} )ã€‚

æ¯”å¦‚ï¼š

```python
W_out = [
  [0.1, 0.2, 0.3],  # å¯¹åº”è¯è¡¨ä¸­ç¬¬0ä¸ªè¯çš„è¾“å‡ºå‘é‡
  [0.4, 0.5, 0.6],  # ç¬¬1ä¸ªè¯
  ...
]
```

---

## ğŸ”„ å››ã€ç»Ÿä¸€è§†è§’ï¼šå‚æ•°çŸ©é˜µ + è¯å‘é‡è¡¨

| è§†è§’     | æ„ä¹‰                                        |
| ------ | ----------------------------------------- |
| ç¥ç»ç½‘ç»œè§’åº¦ | `W_out` æ˜¯ä»éšè—å±‚åˆ°è¾“å‡ºå±‚çš„**æƒé‡çŸ©é˜µ**                |
| è¯è¡¨ç¤ºè§’åº¦  | `W_out[i]` æ˜¯è¯ i ä½œä¸ºä¸Šä¸‹æ–‡è¯æ—¶çš„**è¾“å‡ºåµŒå…¥å‘é‡**        |
| å­¦ä¹ ç›®æ ‡   | è®©ä¸­å¿ƒè¯å‘é‡ `W_in[c]` ä¸ ä¸Šä¸‹æ–‡å‘é‡ `W_out[o]` çš„å†…ç§¯æ›´å¤§ |

æ¢å¥è¯è¯´ï¼š

> Word2Vec è®­ç»ƒçš„å…¶å®æ˜¯ä¸¤ä¸ªè¯å‘é‡è¡¨ï¼š
> ä¸€ä¸ªï¼ˆ`W_in`ï¼‰ç”¨äºâ€œè¯´è¯â€ï¼Œå¦ä¸€ä¸ªï¼ˆ`W_out`ï¼‰ç”¨äºâ€œå¬æ‡‚åˆ«äººè¯´è¯â€ã€‚

---

## ğŸ§  äº”ã€ä¸¾ä¸ªå…·ä½“ä¾‹å­

å‡è®¾è¯è¡¨æœ‰ 3 ä¸ªè¯ï¼Œç»´åº¦ 2ï¼š

```
W_in =
apple   [0.1, 0.2]
data    [0.3, 0.4]
run     [0.5, 0.6]

W_out =
apple   [0.7, 0.8]
data    [0.9, 1.0]
run     [1.1, 1.2]
```

å½“è¾“å…¥ä¸­å¿ƒè¯æ˜¯ `"apple"` æ—¶ï¼š

* ä¸­å¿ƒè¯å‘é‡ï¼š`h = W_in[apple] = [0.1, 0.2]`
* è®¡ç®—å¾—åˆ†ï¼š`u = W_out @ h = [0.23, 0.29, 0.35]`
* softmax(u) å¾—åˆ° `"data"`ã€`"run"`ã€`"apple"` çš„æ¦‚ç‡åˆ†å¸ƒã€‚

> æ¯ä¸ªè¾“å‡ºå‘é‡ï¼ˆW_out çš„ä¸€è¡Œï¼‰éƒ½åƒä¸€ä¸ªâ€œä¸Šä¸‹æ–‡æ¨¡æ¿â€ï¼Œ
> æ¨¡å‹è®¡ç®—â€œå½“å‰ä¸­å¿ƒè¯â€ä¸æ¯ä¸ªæ¨¡æ¿çš„åŒ¹é…ç¨‹åº¦ã€‚

---

## âœ… å…­ã€æ€»ç»“ä¸€å¥è¯

> **`W_out` æ˜¯æƒé‡çŸ©é˜µï¼ˆå› ä¸ºå®ƒè¿æ¥éšè—å±‚å’Œè¾“å‡ºå±‚ï¼‰**ï¼Œ
> **åŒæ—¶å®ƒçš„æ¯ä¸€è¡Œåˆå¯¹åº”ä¸€ä¸ªä¸Šä¸‹æ–‡è¯çš„å‘é‡ï¼ˆå› ä¸ºæ¯ä¸ªè¾“å‡ºç±»åˆ«å°±æ˜¯ä¸€ä¸ªè¯ï¼‰**ã€‚

æ‰€ä»¥å®ƒæ—¢æ˜¯ **å‚æ•°çŸ©é˜µ**ï¼Œä¹Ÿæ˜¯ **è¾“å‡ºè¯å‘é‡è¡¨**ã€‚
è®­ç»ƒç»“æŸåï¼Œæˆ‘ä»¬é€šå¸¸åªå– `W_in`ï¼ˆæˆ–ä¸¤è€…å¹³å‡ï¼‰ä½œä¸ºæœ€ç»ˆè¯å‘é‡ã€‚

---

è¦ä¸è¦æˆ‘å¸®ä½ ç”»ä¸€å¼ å›¾ï¼Œå±•ç¤º `W_in`ã€`W_out` å„è¡Œä»£è¡¨çš„è¯å‘é‡åœ¨å‰å‘ä¼ æ’­ä¸­æ˜¯å¦‚ä½•ä½¿ç”¨çš„ï¼Ÿ
ä¼šéå¸¸ç›´è§‚åœ°çœ‹åˆ°å®ƒä»¬æ—¢æ˜¯çŸ©é˜µåˆæ˜¯è¯è¡¨ã€‚




å¥½çš„ï¼Œæˆ‘ä»¬é‡æ–°ä¸¥æ ¼è®¡ç®—ä¸€é **batch=4, K=5** çš„ä¾‹å­ï¼Œç¡®ä¿è¾“å‡ºå’Œé€»è¾‘æ­£ç¡®ã€‚æˆ‘ä»¬ä¸€æ­¥æ­¥æ¼”ç¤ºã€‚

---

## 1ï¸âƒ£ å‡è®¾ batch æ ·æœ¬

| æ ·æœ¬ç¼–å· | ä¸­å¿ƒè¯       | æ­£æ ·æœ¬ä¸Šä¸‹æ–‡     | è´Ÿæ ·æœ¬5ä¸ª                                      |
| ---- | --------- | ---------- | ------------------------------------------ |
| 0    | "i"       | "love"     | ["deep","pytorch","me","loves","learning"] |
| 1    | "love"    | "i"        | ["pytorch","me","deep","loves","learning"] |
| 2    | "deep"    | "learning" | ["i","love","pytorch","me","loves"]        |
| 3    | "pytorch" | "love"     | ["i","me","deep","learning","loves"]       |

---

## 2ï¸âƒ£ å‡è®¾ç‚¹ç§¯ï¼ˆscoreï¼‰

**æ­£æ ·æœ¬ç‚¹ç§¯ (pos_score)**

```text
pos_score = [0.2, 0.5, 0.3, 0.4]  # shape = [4]
pos_loss = log(sigmoid(pos_score))
```

* sigmoid(x) = 1 / (1 + exp(-x))
* è®¡ç®—ï¼š

| score | sigmoid(score) | log(sigmoid(score)) |
| ----- | -------------- | ------------------- |
| 0.2   | 0.5498         | -0.5991             |
| 0.5   | 0.6225         | -0.4741             |
| 0.3   | 0.5744         | -0.5554             |
| 0.4   | 0.5987         | -0.5130             |

æ‰€ä»¥ï¼š

```text
pos_loss â‰ˆ [-0.599, -0.474, -0.555, -0.513]  # shape=[4]
```

---

## 3ï¸âƒ£ è´Ÿæ ·æœ¬ç‚¹ç§¯ (neg_score)

å‡è®¾æ¯ä¸ªæ ·æœ¬ K=5 ä¸ªè´Ÿæ ·æœ¬ï¼Œéšæœºå‡è®¾ç‚¹ç§¯å¦‚ä¸‹ï¼š

| æ ·æœ¬ | neg_score[5]                |
| -- | --------------------------- |
| 0  | [0.1, -0.2, 0.0, 0.3, -0.1] |
| 1  | [0.2, 0.0, -0.1, 0.1, -0.3] |
| 2  | [-0.1, 0.0, 0.2, -0.2, 0.1] |
| 3  | [0.3, -0.2, 0.1, 0.0, -0.1] |

**è´Ÿæ ·æœ¬æŸå¤±è®¡ç®—ï¼š**

[
neg_loss[i] = \sum_{k=1}^5 \log \sigma(-neg_score[i,k])
]

é€æ ·æœ¬è®¡ç®—è¿‘ä¼¼ï¼š

### æ ·æœ¬0ï¼š

* -neg_score = [-0.1, 0.2, 0.0, -0.3, 0.1]
* sigmoid(-neg_score) â‰ˆ [0.475,0.5498,0.5,0.425,0.525]
* log(sigmoid(-neg_score)) â‰ˆ [-0.746, -0.599, -0.693, -0.856, -0.644]
* sum â‰ˆ -3.538

### æ ·æœ¬1ï¼š

* -neg_score = [-0.2,0.0,0.1,-0.1,0.3]
* sigmoid â‰ˆ [0.450,0.5,0.525,0.475,0.574]
* log â‰ˆ [-0.798,-0.693,-0.645,-0.746,-0.555]
* sum â‰ˆ -3.437

### æ ·æœ¬2ï¼š

* -neg_score = [0.1,0.0,-0.2,0.2,-0.1]
* sigmoid â‰ˆ [0.525,0.5,0.450,0.5498,0.475]
* log â‰ˆ [-0.645,-0.693,-0.798,-0.599,-0.746]
* sum â‰ˆ -3.481

### æ ·æœ¬3ï¼š

* -neg_score = [-0.3,0.2,-0.1,0.0,0.1]
* sigmoid â‰ˆ [0.425,0.5498,0.475,0.5,0.525]
* log â‰ˆ [-0.856,-0.599,-0.746,-0.693,-0.645]
* sum â‰ˆ -3.539

æ‰€ä»¥ï¼š

```text
neg_loss â‰ˆ [-3.538, -3.437, -3.481, -3.539]  # shape=[4]
```

---

## 4ï¸âƒ£ æ¯ä¸ªæ ·æœ¬æ€»æŸå¤±

```text
total_loss_per_sample = pos_loss + neg_loss
â‰ˆ [-0.599 -3.538, -0.474-3.437, -0.555-3.481, -0.513-3.539]
â‰ˆ [-4.137, -3.911, -4.036, -4.052]
```

---

## 5ï¸âƒ£ batch å¹³å‡æŸå¤±

```text
torch.mean(total_loss_per_sample) = (-4.137-3.911-4.036-4.052)/4
â‰ˆ -4.034
final_loss = -torch.mean(...) â‰ˆ 4.034
```

---

### âœ… ç»“æœæ€»ç»“è¡¨æ ¼

| æ ·æœ¬ | pos_loss | neg_loss | total_loss_per_sample |
| -- | -------- | -------- | --------------------- |
| 0  | -0.599   | -3.538   | -4.137                |
| 1  | -0.474   | -3.437   | -3.911                |
| 2  | -0.555   | -3.481   | -4.036                |
| 3  | -0.513   | -3.539   | -4.052                |

* **batch mean = -4.034**
* **æœ€ç»ˆ loss = 4.034**

---

âœ… è¯´æ˜ï¼š

1. pos_loss + neg_loss â†’ æ¯ä¸ªæ ·æœ¬çš„æ€»æŸå¤±
2. K ä¸ªè´Ÿæ ·æœ¬å·²ç»åœ¨ neg_loss å†…éƒ¨ç´¯åŠ 
3. batch å¹³å‡åªé™¤ä»¥ batch_size=4ï¼Œä¸æ˜¯ 4*5

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘å¯ä»¥ç”»ä¸€å¼  **batch=4, K=5 çš„æ­£è´Ÿæ ·æœ¬æŸå¤±ç¤ºæ„å›¾**ï¼Œæ ‡å‡ºæ¯ä¸ªæ ·æœ¬ã€æ­£æ ·æœ¬æŸå¤±ã€è´Ÿæ ·æœ¬æŸå¤±ã€æ€»æŸå¤±å’Œ batch å‡å€¼çš„å…³ç³»ï¼Œè®©é€»è¾‘æ›´ç›´è§‚ã€‚

ä½ å¸Œæœ›æˆ‘ç”»å—ï¼Ÿ



æ˜ç™½ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¹‹å‰çš„ Skip-gram ç¤ºä¾‹é‡Œ **åŠ ä¸Šè´Ÿé‡‡æ ·ï¼ˆNegative Samplingï¼‰**ï¼Œè¿™æ˜¯è®­ç»ƒè¯å‘é‡æ—¶å¸¸ç”¨çš„æ–¹æ³•ï¼Œå¯ä»¥é¿å…å…¨è¯è¡¨ softmax çš„å·¨å¤§è®¡ç®—é‡ã€‚

ä¸‹é¢æˆ‘ç»™ä½ å†™ä¸€ä¸ª **å®Œæ•´å¯è®­ç»ƒç‰ˆ**ï¼š

---

## å®Œæ•´ Skip-gram + è´Ÿé‡‡æ ·ç¤ºä¾‹

```python
import torch
import torch.nn as nn
import torch.optim as optim
import random

# ------------------------
# 1ï¸âƒ£ æ„é€ è¯­æ–™å’Œè¯è¡¨
# ------------------------
sentences = ["i love deep learning", "i love pytorch", "pytorch loves me"]
vocab = list(set(" ".join(sentences).split()))
word2idx = {w: i for i, w in enumerate(vocab)}
idx2word = {i: w for w, i in word2idx.items()}
vocab_size = len(vocab)
print("Vocab:", vocab)

# ------------------------
# 2ï¸âƒ£ ç”Ÿæˆ Skip-gram æ ·æœ¬
# ------------------------
def generate_skipgram_pairs(sentences, window_size=1):
    pairs = []
    for s in sentences:
        words = s.split()
        for i, w in enumerate(words):
            for j in range(max(0, i - window_size), min(len(words), i + window_size + 1)):
                if i != j:
                    pairs.append((w, words[j]))
    return pairs

pairs = generate_skipgram_pairs(sentences)
print("Sample pairs:", pairs[:5])

# ------------------------
# 3ï¸âƒ£ æ¨¡å‹å®šä¹‰
# ------------------------
class SkipGramNegSampling(nn.Module):
    def __init__(self, vocab_size, embed_dim):
        super().__init__()
        self.input_embed = nn.Embedding(vocab_size, embed_dim)
        self.output_embed = nn.Embedding(vocab_size, embed_dim)

    def forward(self, center, pos_context, neg_context):
        """
        center: [batch]
        pos_context: [batch]  æ­£æ ·æœ¬
        neg_context: [batch, K] è´Ÿæ ·æœ¬
        """
        # ä¸­å¿ƒè¯å‘é‡
        center_vec = self.input_embed(center)          # [batch, embed_dim]
        # æ­£æ ·æœ¬å‘é‡
        pos_vec = self.output_embed(pos_context)       # [batch, embed_dim]
        # è´Ÿæ ·æœ¬å‘é‡
        neg_vec = self.output_embed(neg_context)       # [batch, K, embed_dim]

        # --------------------
        # æ­£æ ·æœ¬å¾—åˆ†ï¼ˆç‚¹ç§¯ï¼‰
        # --------------------
        pos_score = torch.sum(center_vec * pos_vec, dim=1)  # [batch]
        pos_loss = torch.log(torch.sigmoid(pos_score) + 1e-10)

        # --------------------
        # è´Ÿæ ·æœ¬å¾—åˆ†
        # --------------------
        # center_vec -> [batch, 1, embed_dim] æ‰©ç»´ä»¥å¹¿æ’­
        center_vec_exp = center_vec.unsqueeze(1)  # [batch, 1, embed_dim]
        neg_score = torch.bmm(neg_vec, center_vec_exp.transpose(1,2)).squeeze(2)  # [batch, K]
        neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-10), dim=1)  # [batch]

        # --------------------
        # æ€» loss
        # --------------------
        return -torch.mean(pos_loss + neg_loss)  # BCE æŸå¤±å–è´Ÿå·

# ------------------------
# 4ï¸âƒ£ è®­ç»ƒå‡†å¤‡
# ------------------------
embed_dim = 8
model = SkipGramNegSampling(vocab_size, embed_dim)
optimizer = optim.Adam(model.parameters(), lr=0.01)

# ------------------------
# 5ï¸âƒ£ è´Ÿé‡‡æ ·å‡½æ•°
# ------------------------
def get_negative_samples(batch_size, K):
    """éšæœºé‡‡æ · K ä¸ªè´Ÿæ ·æœ¬ç´¢å¼•"""
    neg_samples = torch.randint(0, vocab_size, (batch_size, K))
    return neg_samples

# ------------------------
# 6ï¸âƒ£ è®­ç»ƒå¾ªç¯
# ------------------------
epochs = 300
K = 5  # æ¯ä¸ªæ­£æ ·æœ¬é‡‡ 5 ä¸ªè´Ÿæ ·æœ¬

for epoch in range(epochs):
    total_loss = 0
    for center, context in pairs:
        center_idx = torch.tensor([word2idx[center]])
        pos_idx = torch.tensor([word2idx[context]])
        neg_idx = get_negative_samples(1, K)  # batch_size=1

        loss = model(center_idx, pos_idx, neg_idx)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
    if epoch % 50 == 0:
        print(f"Epoch {epoch}, Loss={total_loss:.4f}")

# ------------------------
# 7ï¸âƒ£ æŸ¥çœ‹å­¦åˆ°çš„è¯å‘é‡
# ------------------------
for word in vocab:
    vec = model.input_embed.weight[word2idx[word]].detach().numpy()
    print(word, "â†’", vec)
```

---

## âœ… è¯´æ˜

1. **æ­£æ ·æœ¬**

   * æ¯ä¸ª `(center, context)` å¯¹æ˜¯æ­£æ ·æœ¬ â†’ æœŸæœ›ç‚¹ç§¯å¤§ â†’ `sigmoid(score) â‰ˆ 1`

2. **è´Ÿæ ·æœ¬**

   * æ¯ä¸ªæ­£æ ·æœ¬éšæœºæŠ½ K ä¸ªè¯ä½œä¸ºè´Ÿæ ·æœ¬ â†’ æœŸæœ›ç‚¹ç§¯å° â†’ `sigmoid(-score) â‰ˆ 1`

3. **Loss**
   [
   \mathcal{L} = - \sum_{(c, w^+)} \log \sigma(c \cdot w^+) - \sum_{(c, w^-)} \sum_{k=1}^K \log \sigma(-c \cdot w^-_k)
   ]

4. **ä¼˜ç‚¹**

   * è®­ç»ƒé€Ÿåº¦å¿«ï¼Œé¿å…å…¨è¯è¡¨ softmax
   * æ ·æœ¬å°‘æ—¶ä¹Ÿèƒ½å­¦åˆ°åˆç†è¯å‘é‡

---

æˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€å¼  **æ­£æ ·æœ¬/è´Ÿæ ·æœ¬ç‚¹ç§¯ â†’ sigmoid â†’ loss â†’ embedding æ›´æ–°** çš„æµç¨‹å›¾ï¼Œè®©è´Ÿé‡‡æ ·åŸç†ä¸€çœ¼çœ‹æ‡‚ã€‚

ä½ å¸Œæœ›æˆ‘ç”»å—ï¼Ÿ

